{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Statistics Part 2\n",
        "\n",
        "Q1 What is hypothesis testing in statistics ?\n",
        "\n",
        "Hypothesis testing is a statistical method used to make decisions or inferences about a population based on sample data. It helps determine whether there is enough evidence to support a certain claim or assumption.\n",
        "\n",
        "Here's how it works:\n",
        "1. **State the hypotheses**: You define two hypotheses—\n",
        "   - **Null hypothesis (H₀)**: This represents the default assumption or status quo (e.g., \"There is no difference between two treatments\").\n",
        "   - **Alternative hypothesis (H₁ or Hₐ)**: This suggests an effect or difference exists (e.g., \"Treatment A is more effective than Treatment B\").\n",
        "\n",
        "2. **Choose a significance level (α)**: Typically, a threshold like 0.05 or 0.01 is set. This represents the probability of rejecting the null hypothesis when it is actually true (Type I error).\n",
        "\n",
        "3. **Collect and analyze data**: A statistical test (e.g., t-test, chi-square test, ANOVA) is used to compare observed data against what would be expected under H₀.\n",
        "\n",
        "4. **Compute the test statistic and p-value**: The test statistic helps measure how extreme the data is under H₀. The p-value indicates the probability of obtaining results as extreme as the observed ones if H₀ were true.\n",
        "\n",
        "5. **Make a decision**:\n",
        "   - If the p-value is smaller than α, you reject H₀, suggesting evidence supports H₁.\n",
        "   - If the p-value is larger, you fail to reject H₀, meaning there isn’t enough evidence to support H₁.\n",
        "\n",
        "Q2 What is the null hypothesis, and how does it differ from the alternative hypothesis ?\n",
        "\n",
        "The **null hypothesis (H₀) is the default assumption that there is no effect, no difference, or no relationship between variables. It represents the status quo or what is assumed to be true unless there is enough evidence to prove otherwise. For example, in a medical trial, H₀ could be: *\"This new drug has no effect on blood pressure.\"*\n",
        "\n",
        "The **alternative hypothesis (H₁ or Hₐ)**, on the other hand, proposes that there is an effect, a difference, or a relationship. It challenges the null hypothesis and suggests that something significant is happening. In the same medical trial example, H₁ could be: *\"This new drug lowers blood pressure.\"*\n",
        "\n",
        "Key Differences:\n",
        "1. **Purpose**:  \n",
        "   - H₀ assumes no change or relationship exists.  \n",
        "   - H₁ suggests that there is a change or effect.  \n",
        "\n",
        "2. **Decision Making**:  \n",
        "   - If statistical evidence strongly contradicts H₀, it is rejected in favor of H₁.  \n",
        "   - If evidence is insufficient, H₀ is not rejected, meaning we don’t have proof to support H₁.\n",
        "\n",
        "3. **Risk of Error**:  \n",
        "   - Rejecting H₀ when it is actually true is a **Type I error** (false positive).  \n",
        "   - Failing to reject H₀ when H₁ is actually true is a **Type II error** (false negative).  \n",
        "\n",
        "Q3 What is the significance level in hypothesis testing, and why is it important ?\n",
        "\n",
        "The **significance level (α)** in hypothesis testing is the probability of rejecting the null hypothesis (H₀) when it is actually true. It sets a threshold for determining whether the observed results are statistically significant.\n",
        "\n",
        "Why Is It Important?\n",
        "1. **Controls Error Risk**: Since hypothesis testing involves probability, there’s always a chance of making a mistake. Setting a significance level helps manage the risk of **Type I error** (incorrectly rejecting H₀ when it’s true).\n",
        "2. **Defines Statistical Confidence**: A lower α (e.g., 0.01) demands stronger evidence before rejecting H₀, while a higher α (e.g., 0.10) allows more flexibility in making conclusions.\n",
        "3. **Standardization in Research**: Common values like **0.05 (5%)** or **0.01 (1%)** are widely used to ensure consistency across scientific studies and data analysis.\n",
        "4. **Influences Decision Making**: If the **p-value** (computed probability) is **less than α**, we reject H₀, suggesting strong evidence for the alternative hypothesis (H₁). If **p ≥ α**, we fail to reject H₀.\n",
        "\n",
        "Q4 What does a P-value represent in hypothesis testing ?\n",
        "\n",
        "The **p-value** in hypothesis testing represents the probability of obtaining results as extreme as the observed ones, assuming the **null hypothesis (H₀)** is true. It helps assess whether the observed data provides strong enough evidence to reject H₀ in favor of the **alternative hypothesis (H₁).**\n",
        "\n",
        "How to interpret a p-value:\n",
        "- **Small p-value (≤ α, such as 0.05 or 0.01)**: Strong evidence against H₀ → **Reject H₀** (supporting H₁).\n",
        "- **Large p-value (> α)**: Not enough evidence to reject H₀ → **Fail to reject H₀** (no strong support for H₁).\n",
        "\n",
        "Example:\n",
        "\n",
        "Imagine a study tests whether a new fertilizer improves plant growth.\n",
        "- **H₀**: The fertilizer has no effect.\n",
        "- **H₁**: The fertilizer increases plant growth.\n",
        "- Researchers set **α = 0.05** and obtain **p = 0.02**.\n",
        "\n",
        "Q5 How do you interpret the P-value in hypothesis testing ?\n",
        "\n",
        "Interpreting the **p-value** in hypothesis testing helps determine whether the observed data provides enough evidence to reject the **null hypothesis (H₀)** in favor of the **alternative hypothesis (H₁).** Here's how to approach it:\n",
        "\n",
        " **Interpreting the p-value:**\n",
        "1. **Low p-value (≤ α, e.g., 0.05 or 0.01)**  \n",
        "   - Suggests that the observed data is unlikely to have occurred under H₀.  \n",
        "   - Provides strong evidence **against** H₀ → **Reject H₀** → Supports H₁.  \n",
        "\n",
        "2. **High p-value (> α)**  \n",
        "   - Implies that the data is more consistent with H₀, meaning no strong evidence exists to reject it.  \n",
        "   - **Fail to reject H₀** → Not enough support for H₁.  \n",
        "\n",
        " **Example Interpretation:**  \n",
        "Imagine researchers test whether a new teaching method improves student performance.  \n",
        "- **H₀**: The teaching method has no effect.  \n",
        "- **H₁**: The teaching method improves performance.  \n",
        "- **Significance level (α) = 0.05**  \n",
        "- **Obtained p-value = 0.02**  \n",
        "\n",
        "Q6 What are Type 1 and Type 2 errors in hypothesis testing ?\n",
        "\n",
        "In hypothesis testing, **Type I and Type II errors** refer to mistakes in the decision-making process when evaluating a hypothesis. These errors occur because hypothesis testing is based on probabilities, meaning there's always a chance of making the wrong conclusion.\n",
        "\n",
        " **Type I Error (False Positive)**\n",
        "- Occurs when the **null hypothesis (H₀) is rejected when it is actually true**.\n",
        "- Essentially, you detect an effect or difference when none exists.\n",
        "- Probability of making a Type I error is **α** (the significance level, e.g., 0.05 or 0.01).\n",
        "\n",
        "**Example:**  \n",
        "A medical test wrongly detects a disease in a healthy person.\n",
        "\n",
        " **Type II Error (False Negative)**\n",
        "- Happens when **H₀ is not rejected when the alternative hypothesis (H₁) is actually true**.\n",
        "- You fail to detect a real effect or difference.\n",
        "- Probability of making a Type II error is **β**, and (1 - β) represents the **power** of the test (the ability to detect real effects).\n",
        "\n",
        "**Example:**  \n",
        "A medical test fails to detect a disease in a sick person.\n",
        "\n",
        " **Balancing Errors**\n",
        "- Lowering **α** reduces Type I errors but increases the risk of Type II errors.\n",
        "- Increasing the sample size helps reduce both errors, leading to more reliable conclusions.\n",
        "\n",
        "Q7 What is the difference between a one-tailed and a two-tailed test in hypothesis testing ?\n",
        "\n",
        "The key difference between a **one-tailed** and a **two-tailed** test in hypothesis testing lies in the directionality of the test.\n",
        "\n",
        "**One-Tailed Test** (Directional)\n",
        "- Used when the research hypothesis specifies **a direction** of the effect.\n",
        "- It tests for an **increase or decrease** in a parameter, not both.\n",
        "- The critical region (rejection area for H₀) is only on **one side** of the distribution.\n",
        "\n",
        "**Example**:\n",
        "\n",
        "A company claims its new battery lasts **more than** 10 hours.  \n",
        "- **H₀**: The battery lasts **≤ 10 hours**.  \n",
        "- **H₁**: The battery lasts **> 10 hours** (right-tailed).  \n",
        "- The test will only check for whether the battery life is significantly **greater** than 10 hours.\n",
        "\n",
        "**Two-Tailed Test** (Non-Directional)\n",
        "- Used when the research hypothesis does **not** specify a direction.\n",
        "- It checks for **any significant difference**—whether the parameter is higher or lower.\n",
        "- The critical region is **split between both tails** of the distribution.\n",
        "\n",
        "**Example**:  \n",
        "A pharmaceutical company wants to test if a drug **affects** blood pressure in **either** direction.  \n",
        "- **H₀**: The drug has **no effect** on blood pressure.  \n",
        "- **H₁**: The drug **changes** blood pressure (either increases or decreases).  \n",
        "- The test examines **both** possibilities (higher or lower blood pressure).\n",
        "\n",
        "**Choosing Between the Two**\n",
        "- Use a **one-tailed test** if you're only interested in a specific direction (e.g., \"greater than\" or \"less than\").\n",
        "- Use a **two-tailed test** when looking for **any change** or difference (e.g., \"not equal to\").\n",
        "\n",
        "Q8 What is the Z-test, and when is it used in hypothesis testing ?\n",
        "\n",
        "The **Z-test** is a statistical test used to determine whether there is a significant difference between sample and population means or between two sample means, assuming the data follows a normal distribution. It relies on the **Z-score**, which measures how far a sample statistic deviates from the expected population parameter.\n",
        "\n",
        " **When is the Z-test used?**\n",
        "A Z-test is applied when:\n",
        "1. **Sample Size is Large** (typically **n ≥ 30**).  \n",
        "2. **Data is Normally Distributed** (or the sample size is large enough for the Central Limit Theorem to apply).  \n",
        "3. **Population Variance is Known** (or assumed).  \n",
        "4. **Comparing a Sample Mean to a Population Mean** or **Comparing Two Independent Samples**.  \n",
        "\n",
        " **Types of Z-tests**\n",
        "1. **One-sample Z-test**: Used to compare a sample mean with a known population mean.  \n",
        "   *Example*: Checking if the average height of students in a school differs from the national average.  \n",
        "2. **Two-sample Z-test**: Used to compare the means of two independent samples.  \n",
        "   *Example*: Comparing the average test scores of students from two different schools.  \n",
        "3. **Z-test for Proportions**: Used when dealing with categorical data and comparing proportions.  \n",
        "   *Example*: Testing whether the proportion of people preferring a new product differs from past preferences.  \n",
        "\n",
        "**Formula for One-Sample Z-Test**\n",
        "\\[\n",
        "Z = \\frac{\\bar{X} - \\mu}{\\frac{\\sigma}{\\sqrt{n}}}\n",
        "\\]\n",
        "Where:\n",
        "- \\( \\bar{X} \\) = sample mean  \n",
        "- \\( \\mu \\) = population mean  \n",
        "- \\( \\sigma \\) = population standard deviation  \n",
        "- \\( n \\) = sample size  \n",
        "\n",
        "Q9 How do you calculate the Z-score, and what does it represent in hypothesis testing ?\n",
        "\n",
        "The **Z-score**, also known as the **standard score**, measures how far a data point or sample mean deviates from the population mean in terms of **standard deviations**. It helps determine whether the observed data is significantly different from the expected norm.\n",
        "\n",
        "**Formula for Z-score:**\n",
        "For a single data point:\n",
        "\\[\n",
        "Z = \\frac{X - \\mu}{\\sigma}\n",
        "\\]\n",
        "For a sample mean in hypothesis testing:\n",
        "\\[\n",
        "Z = \\frac{\\bar{X} - \\mu}{\\frac{\\sigma}{\\sqrt{n}}}\n",
        "\\]\n",
        "Where:\n",
        "- \\( X \\) = individual data value\n",
        "- \\( \\bar{X} \\) = sample mean  \n",
        "- \\( \\mu \\) = population mean  \n",
        "- \\( \\sigma \\) = population standard deviation  \n",
        "- \\( n \\) = sample size  \n",
        "\n",
        "**What Does the Z-score Represent?**\n",
        "1. **Measures Distance from Mean**:  \n",
        "   - Positive Z-score → Data is above the mean  \n",
        "   - Negative Z-score → Data is below the mean  \n",
        "\n",
        "2. **Indicates Significance in Hypothesis Testing**:  \n",
        "   - If the Z-score is extreme (far from zero), it suggests the sample mean significantly differs from the population mean.  \n",
        "   - The Z-score is compared against critical values from the **Z-table** (often **±1.96** for a 95% confidence level).  \n",
        "\n",
        "Q10  What is the T-distribution, and when should it be used instead of the normal distribution ?\n",
        "\n",
        "The **T-distribution** (Student's T-distribution) is a probability distribution that is similar to the **normal distribution** but has heavier tails. This means it accounts for more variability in data, making it useful when dealing with small sample sizes or unknown population variances.\n",
        "\n",
        "**When to Use the T-Distribution Instead of the Normal Distribution?**\n",
        "1. **Small Sample Size (\\( n < 30 \\))**:  \n",
        "   - When the sample size is small, the normal distribution may not accurately represent the population. The T-distribution adjusts for this by considering sample variability.\n",
        "   \n",
        "2. **Unknown Population Standard Deviation (\\( \\sigma \\))**:  \n",
        "   - If the **population standard deviation** is unknown, we estimate it using the **sample standard deviation (s)**. The T-distribution provides better estimates in such cases.\n",
        "   \n",
        "3. **More Variability and Uncertainty**:  \n",
        "   - Since the T-distribution has wider tails than the normal distribution, it accounts for more variability and **reduces the risk of underestimating uncertainty** when making conclusions.\n",
        "\n",
        "**Key Differences Between T-Distribution and Normal Distribution**\n",
        "\n",
        "| Feature            | Normal Distribution | T-Distribution |\n",
        "|-------------------|----------------|--------------|\n",
        "| **Shape**         | Bell-shaped, thin tails | Bell-shaped, but heavier tails |\n",
        "| **Usage**         | Large samples (\\( n \\geq 30 \\)) | Small samples (\\( n < 30 \\)) |\n",
        "| **Standard Deviation** | Known (\\( \\sigma \\)) | Unknown, estimated from sample |\n",
        "| **Tail Thickness** | Thinner tails | Thicker tails (accounts for more variability) |\n",
        "\n",
        "Q11  What is the difference between a Z-test and a T-test ?\n",
        "\n",
        "The **Z-test** and **T-test** are both statistical tests used to compare sample data to a population or between two samples. The key difference lies in when they should be applied.\n",
        "\n",
        "**Key Differences:**\n",
        "\n",
        "| Feature | Z-test | T-test |\n",
        "|---------|-------|-------|\n",
        "| **Sample Size** | Large (\\( n \\geq 30 \\)) | Small (\\( n < 30 \\)) |\n",
        "| **Population Standard Deviation (\\( \\sigma \\))** | Known | Unknown (uses sample standard deviation \\( s \\)) |\n",
        "| **Distribution Used** | Normal distribution | T-distribution (heavier tails for more variability) |\n",
        "| **Typical Use Cases** | Comparing means or proportions in large datasets | Comparing means in small datasets where population parameters are unknown |\n",
        "\n",
        "**When to Use Each:**\n",
        "- **Use a Z-test** when the sample is large, normally distributed, and the population standard deviation is known.\n",
        "- **Use a T-test** when the sample is small and/or the population standard deviation is unknown.\n",
        "\n",
        " **Example Scenarios:**\n",
        "\n",
        "**Z-test:**  \n",
        "A researcher wants to test whether the average height of students in a university (large sample) differs from the national average. The **population standard deviation is known**, so a Z-test is appropriate.\n",
        "\n",
        "**T-test:**  \n",
        "A company tests whether a new training program improves employee productivity. They only have **15 employees**, and the population standard deviation is unknown. A **T-test** is more suitable.\n",
        "\n",
        "Q12 What is the T-test, and how is it used in hypothesis testing ?\n",
        "\n",
        "The **T-test** is a statistical test used to determine whether there is a significant difference between the means of two groups. It is particularly useful when dealing with **small sample sizes (\\( n < 30 \\))** and when the **population standard deviation (\\( \\sigma \\)) is unknown**.\n",
        "\n",
        "**How the T-test is Used in Hypothesis Testing:**\n",
        "1. **State the Hypotheses**:  \n",
        "   - **Null Hypothesis (H₀)**: Assumes no difference between group means.  \n",
        "   - **Alternative Hypothesis (H₁)**: Suggests there is a significant difference.\n",
        "\n",
        "2. **Choose the Type of T-test**:  \n",
        "   - **One-sample T-test**: Compares a sample mean to a known population mean.  \n",
        "   - **Independent (two-sample) T-test**: Compares means of two independent groups.  \n",
        "   - **Paired T-test**: Compares means from the same group before and after treatment.\n",
        "\n",
        "3. **Calculate the T-score**:\n",
        "\\[\n",
        "T = \\frac{\\bar{X}_1 - \\bar{X}_2}{\\frac{s}{\\sqrt{n}}}\n",
        "\\]\n",
        "Where:A What is the relationship between Z-test and T-test in hypothesis testing,\n",
        "- \\( \\bar{X}_1, \\bar{X}_2 \\) = sample means  \n",
        "- \\( s \\) = estimated standard deviation  \n",
        "- \\( n \\) = sample size  \n",
        "\n",
        "4. **Compare the T-score to the Critical Value from the T-table**:  \n",
        "   - If the **computed T-value exceeds the critical value**, reject **H₀**.  \n",
        "   - If it falls within the acceptance range, **fail to reject H₀**.\n",
        "\n",
        "\n",
        "Q13  What is the relationship between Z-test and T-test in hypothesis testing ?\n",
        "\n",
        "The **Z-test** and **T-test** are closely related statistical tests used in hypothesis testing to compare sample data with population parameters or between two sample groups. While both tests assess whether observed differences are statistically significant, their application differs based on sample size and available data.\n",
        "\n",
        "**Relationship Between Z-test and T-test**\n",
        "1. **Purpose**:  \n",
        "   - Both tests evaluate whether a sample mean significantly differs from a population mean or another sample mean.\n",
        "   - They help determine whether to **reject the null hypothesis (H₀)** in favor of the alternative hypothesis (H₁).\n",
        "\n",
        "2. **Underlying Distribution**:  \n",
        "   - **Z-test** relies on the **normal distribution** (used when the sample size is large).  \n",
        "   - **T-test** uses the **T-distribution**, which accounts for **greater variability** in smaller samples.  \n",
        "\n",
        "3. **Sample Size Dependency**:  \n",
        "   - **Z-test** is appropriate when **\\( n \\geq 30 \\)** (large sample).  \n",
        "   - **T-test** is used for **\\( n < 30 \\)** (small sample) because it adjusts for **sample variability** with heavier tails.  \n",
        "\n",
        "4. **Known vs. Unknown Population Standard Deviation (\\( \\sigma \\))**:  \n",
        "   - **Z-test** is used when the **population standard deviation** (\\( \\sigma \\)) is known.  \n",
        "   - **T-test** is used when **\\( \\sigma \\)** is unknown and must be estimated from sample data.  \n",
        "\n",
        " **Common Usage of Both Tests**\n",
        "\n",
        "| Feature | Z-test | T-test |\n",
        "|---------|-------|--------|\n",
        "| **Distribution Used** | Normal | T-distribution |\n",
        "| **Sample Size** | Large (\\( n \\geq 30 \\)) | Small (\\( n < 30 \\)) |\n",
        "| **Standard Deviation (\\( \\sigma \\))** | Known | Unknown (estimated from sample) |\n",
        "| **Use Case** | Large datasets, population-level analysis | Small datasets, experimental studies |\n",
        "\n",
        "Q14 What is a confidence interval, and how is it used to interpret statistical results ?\n",
        "\n",
        "A **confidence interval (CI)** is a range of values that provides an estimate of where a population parameter (such as a mean or proportion) is likely to fall, based on sample data. It reflects the uncertainty in statistical analysis and helps determine the reliability of results.\n",
        "\n",
        "**How Confidence Intervals Are Used in Statistical Interpretation**\n",
        "\n",
        "1. **Indicates Precision**  \n",
        "   - A **narrow** confidence interval suggests precise estimates.  \n",
        "   - A **wide** confidence interval indicates more uncertainty.  \n",
        "\n",
        "2. **Helps Make Decisions**  \n",
        "   - If a CI does **not** include a specific value (e.g., zero for differences), it suggests a statistically **significant** result.  \n",
        "   - If a CI **includes** the null hypothesis value, the result might not be statistically significant.  \n",
        "\n",
        "3. **Provides a Confidence Level**  \n",
        "   - Common values: **90%, 95%, and 99%** confidence levels.  \n",
        "   - A **95% CI** means: \"If we repeated the study multiple times, 95% of the time, the true population parameter would fall within this range.\"  \n",
        "\n",
        "**Confidence Interval Formula (for Mean)**\n",
        "\n",
        "\\[\n",
        "CI = \\bar{X} \\pm Z \\times \\frac{\\sigma}{\\sqrt{n}}\n",
        "\\]\n",
        "Where:\n",
        "- \\( \\bar{X} \\) = sample mean  \n",
        "- \\( Z \\) = critical value from Z-table (based on confidence level)  \n",
        "- \\( \\sigma \\) = population standard deviation (or estimated from sample)  \n",
        "- \\( n \\) = sample size  \n",
        "\n",
        "Q15  What is the margin of error, and how does it affect the confidence interval ?\n",
        "\n",
        "The **margin of error (MoE)** is the range within which the true population parameter is expected to fall, based on a sample estimate. It represents the level of uncertainty in statistical results and directly affects the **confidence interval (CI)** by determining how wide or narrow it is.\n",
        "\n",
        "**How the Margin of Error Affects the Confidence Interval**\n",
        "\n",
        "1. **Defines the Range of Possible Values**  \n",
        "   - The confidence interval is calculated as:  \n",
        "     \\[\n",
        "     CI = \\bar{X} \\pm \\text{Margin of Error}\n",
        "     \\]\n",
        "   - A **larger MoE** means the confidence interval is wider, indicating **more uncertainty** in the estimate.\n",
        "   - A **smaller MoE** means the confidence interval is narrower, suggesting **greater precision** in the estimate.\n",
        "\n",
        "2. **Factors Influencing the Margin of Error**\n",
        "   - **Confidence Level**: Higher confidence levels (e.g., 99% vs. 95%) lead to a **larger MoE**.\n",
        "   - **Sample Size (\\( n \\))**: A larger sample size reduces variability, leading to a **smaller MoE**.\n",
        "   - **Standard Deviation (\\( \\sigma \\))**: More variability in data increases MoE.\n",
        "   - **Critical Value (Z or t-score)**: Based on statistical tables for normal or T-distributions.\n",
        "\n",
        "**Formula for Margin of Error**\n",
        "\n",
        "\\[\n",
        "\\text{MoE} = Z \\times \\frac{\\sigma}{\\sqrt{n}}\n",
        "\\]\n",
        "Where:\n",
        "- \\( Z \\) = critical value from Z-table (for normal distribution)\n",
        "- \\( \\sigma \\) = population standard deviation (or estimated from sample)\n",
        "- \\( n \\) = sample size\n",
        "\n",
        "**Example Interpretation**\n",
        "\n",
        "Imagine a survey estimates that **60% of customers prefer a new product**, with a **margin of error of ±3%** at **95% confidence**.\n",
        "\n",
        "- **Confidence Interval**: (57%, 63%)  \n",
        "- This means researchers are **95% confident** that the true percentage of customer preference falls within this range.\n",
        "\n",
        "Q16 How is Bayes' Theorem used in statistics, and what is its significance ?\n",
        "\n",
        "Bayes' Theorem is a fundamental concept in statistics that describes how to update the probability of an event based on new evidence. It is widely used in probability theory, machine learning, and decision-making processes.\n",
        "\n",
        "**Bayes' Theorem Formula**\n",
        "\n",
        "\\[\n",
        "P(A|B) = \\frac{P(B|A) \\cdot P(A)}{P(B)}\n",
        "\\]\n",
        "Where:\n",
        "- \\( P(A|B) \\) = Probability of **A** given that **B** has occurred (posterior probability).\n",
        "- \\( P(B|A) \\) = Probability of **B** given **A** (likelihood).\n",
        "- \\( P(A) \\) = Prior probability of **A**.\n",
        "- \\( P(B) \\) = Probability of observing **B** (marginal probability).\n",
        "\n",
        "**Significance of Bayes' Theorem**\n",
        "\n",
        "1. **Updates Beliefs with New Information**  \n",
        "   - It helps refine probabilities based on observed data.\n",
        "   - Used in **medical diagnosis**, where prior knowledge of disease prevalence is updated with new test results.\n",
        "\n",
        "2. **Decision-Making Under Uncertainty**  \n",
        "   - Applied in **risk analysis, finance, and machine learning models** to evaluate uncertainty.\n",
        "\n",
        "3. **Spam Filtering & AI Predictions**  \n",
        "   - Used in email spam filters, where prior probabilities of spam emails are adjusted based on characteristics like keywords.\n",
        "\n",
        "4. **Genetics & Medicine**  \n",
        "   - Helps assess the probability of genetic disorders based on family history and test results.\n",
        "\n",
        "**Example Application: Medical Diagnosis**\n",
        "\n",
        "A doctor wants to determine whether a patient has a rare disease.  \n",
        "- **Prior Probability**: The disease affects 1% of the population.  \n",
        "- **Test Accuracy**: A positive test correctly detects the disease 95% of the time, but has a 5% false positive rate.  \n",
        "- **Bayes' Theorem** helps the doctor update the probability based on a **positive test result**.\n",
        "\n",
        "Q17 What is the Chi-square distribution, and when is it used ?\n",
        "\n",
        "The **Chi-square distribution** is a probability distribution widely used in statistics for hypothesis testing, especially when dealing with categorical data. It arises when summing the squares of independent standard normal variables and is primarily used in goodness-of-fit tests, independence tests, and variance analysis.\n",
        "\n",
        "**Key Characteristics of Chi-square Distribution:**\n",
        "\n",
        "- **Skewed Distribution:** Positively skewed, but becomes more symmetric as degrees of freedom increase.\n",
        "- **Non-Negative Values:** Since it represents squared quantities, Chi-square values are always **≥ 0**.\n",
        "- **Degrees of Freedom (df):** Determines its shape and depends on the number of categories or independent variables.\n",
        "\n",
        "**When is the Chi-square Distribution Used?**\n",
        "\n",
        "1. **Chi-square Goodness-of-Fit Test:**  \n",
        "   - Checks if a sample distribution matches an expected distribution.  \n",
        "   - Example: Determining whether observed customer preferences follow predicted proportions.\n",
        "\n",
        "2. **Chi-square Test for Independence:**  \n",
        "   - Assesses whether two categorical variables are independent.  \n",
        "   - Example: Testing if gender influences product preference.\n",
        "\n",
        "3. **Chi-square Test for Variance:**  \n",
        "   - Determines if a population variance differs from a specified value.  \n",
        "   - Example: Checking whether production quality variance meets industry standards.\n",
        "\n",
        "**Chi-square Formula for Independence Test**\n",
        "\n",
        "\\[\n",
        "\\chi^2 = \\sum \\frac{(O - E)^2}{E}\n",
        "\\]\n",
        "Where:\n",
        "- \\( O \\) = Observed frequency  \n",
        "- \\( E \\) = Expected frequency  \n",
        "\n",
        "**Example Application:**\n",
        "\n",
        "Suppose a company wants to check if customer satisfaction is **independent** of location. They survey **customers across multiple cities** and conduct a **Chi-square test** to see if satisfaction levels differ significantly.\n",
        "\n",
        "Q18 What is the Chi-square goodness of fit test, and how is it applied ?\n",
        "\n",
        "The **Chi-square goodness-of-fit test** is a statistical test used to determine whether a sample distribution differs significantly from an expected or theoretical distribution. It helps assess how well observed data matches a predicted pattern.\n",
        "\n",
        "**How the Chi-square Goodness-of-Fit Test Works:**\n",
        "\n",
        "1. **Define Hypotheses**  \n",
        "   - **Null Hypothesis (H₀)**: The observed distribution **matches** the expected distribution.  \n",
        "   - **Alternative Hypothesis (H₁)**: The observed distribution **does not** match the expected distribution.  \n",
        "\n",
        "2. **Collect Data and Define Expected Values**  \n",
        "   - The expected values are determined based on a known theoretical distribution or historical data.  \n",
        "\n",
        "3. **Apply the Chi-square Formula:**  \n",
        "\\[\n",
        "\\chi^2 = \\sum \\frac{(O - E)^2}{E}\n",
        "\\]\n",
        "Where:\n",
        "- \\( O \\) = Observed frequency  \n",
        "- \\( E \\) = Expected frequency  \n",
        "\n",
        "4. **Compare the Chi-square Statistic to the Critical Value:**  \n",
        "   - Use a **Chi-square distribution table** to find the critical value for a chosen **significance level (α, usually 0.05)** and **degrees of freedom (df = categories - 1)**.  \n",
        "   - If **computed \\( \\chi^2 \\)** is greater than the table value, **reject H₀**, meaning the data does not fit the expected distribution.  \n",
        "\n",
        "Q19 What is the F-distribution, and when is it used in hypothesis testing ?\n",
        "\n",
        "The **F-distribution** is a probability distribution that arises frequently in statistical analysis, particularly in hypothesis testing involving **variance comparisons**. It is **right-skewed** and only takes **positive values** since it deals with squared quantities.\n",
        "\n",
        "**When is the F-Distribution Used?**\n",
        "\n",
        "The F-distribution is used when comparing **ratios of variances** in different scenarios:\n",
        "\n",
        "1. **Analysis of Variance (ANOVA)**  \n",
        "   - Determines whether the means of multiple groups are **significantly different**.\n",
        "   - Example: A company tests whether employee productivity differs across three departments.\n",
        "\n",
        "2. **F-test for Comparing Variances**  \n",
        "   - Checks if two population variances are **equal**.\n",
        "   - Example: Comparing variability in product defects between two manufacturing processes.\n",
        "\n",
        "3. **Regression Analysis**  \n",
        "   - Helps evaluate the **overall significance** of regression models.\n",
        "   - Example: Assessing whether multiple predictors explain variation in sales data.\n",
        "\n",
        "**Formula for the F-statistic**\n",
        "\n",
        "\\[\n",
        "F = \\frac{\\text{Variance of Sample 1}}{\\text{Variance of Sample 2}}\n",
        "\\]\n",
        "Where:\n",
        "- A **large F-value** suggests significant differences in variances.\n",
        "- Critical values are obtained from the **F-distribution table**, based on **degrees of freedom** (\\( df \\)).\n",
        "\n",
        "The **F-distribution** plays a crucial role in statistical hypothesis testing whenever variance-based comparisons are required.\n",
        "\n",
        "Q20 What is an ANOVA test, and what are its assumptions ?\n",
        "\n",
        "The **Analysis of Variance (ANOVA)** test is a statistical method used to compare the means of multiple groups to determine if there is a **significant difference** among them. Unlike a t-test, which compares two groups, ANOVA allows for comparison across three or more groups.\n",
        "\n",
        " **How ANOVA Works:**\n",
        "\n",
        "1. **Form Hypotheses:**  \n",
        "   - **Null Hypothesis (H₀)**: All group means are equal (no significant difference).  \n",
        "   - **Alternative Hypothesis (H₁)**: At least one group mean is different.  \n",
        "\n",
        "2. **Calculate the F-statistic:**  \n",
        "   - Measures the variance **between** groups and compares it to variance **within** groups.  \n",
        "   - A large F-value suggests that group means **differ significantly**.\n",
        "\n",
        "3. **Compare to Critical Value:**  \n",
        "   - If the computed **F-value** exceeds the critical value from the F-distribution table, we reject **H₀**.\n",
        "\n",
        "**Types of ANOVA Tests**\n",
        "\n",
        "- **One-way ANOVA:** Compares **one independent variable** across multiple groups.  \n",
        "   Example: Examining whether three different teaching methods impact student performance.  \n",
        "- **Two-way ANOVA:** Analyzes the effects of **two independent variables** simultaneously.  \n",
        "  Example: Studying how **teaching method and gender** affect student performance.  \n",
        "\n",
        "**Assumptions of ANOVA**\n",
        "\n",
        "To ensure valid results, ANOVA relies on these assumptions:\n",
        "1. **Independence:** Observations are independent of each other.  \n",
        "2. **Normality:** Data in each group follows a **normal distribution** (especially important for small sample sizes).  \n",
        "3. **Homogeneity of Variance (Homoscedasticity):** The variance across groups should be approximately **equal** (tested using Levene’s test).  \n",
        "\n",
        "If assumptions are violated, alternative methods like **Welch’s ANOVA** or **Kruskal-Wallis test** may be used.\n",
        "\n",
        "Q21 What are the different types of ANOVA tests ?\n",
        "\n",
        "There are several types of **ANOVA (Analysis of Variance) tests**, each designed for different experimental setups and comparisons:\n",
        "\n",
        " **1. One-Way ANOVA**  \n",
        "- Compares **means of three or more groups** based on a **single independent variable**.  \n",
        "- Used when testing the effect of one factor on a dependent variable.  \n",
        "🔹 *Example:* Comparing the effectiveness of **three different teaching methods** on student performance.  \n",
        "\n",
        "**2. Two-Way ANOVA**  \n",
        "- Compares the means of groups based on **two independent variables**.  \n",
        "- Examines whether each factor has a significant effect and whether they interact.  \n",
        "🔹 *Example:* Studying how **exercise type and diet** influence weight loss.  \n",
        "\n",
        "**3. Repeated Measures ANOVA**  \n",
        "- Used when measuring **the same subjects multiple times** under different conditions.  \n",
        "- Helps analyze changes **over time** within the same participants.  \n",
        "🔹 *Example:* Tracking **employee productivity** before, during, and after a training program.  \n",
        "\n",
        " **4. Mixed-Design ANOVA**  \n",
        "- Combines elements of **repeated measures** and **two-way ANOVA**.  \n",
        "- Used for studies that involve **both within-subject and between-subject comparisons**.  \n",
        " *Example:* Testing how **a new medication affects different age groups** over time.  \n",
        "\n",
        "**5. MANOVA (Multivariate ANOVA)**  \n",
        "- Extends ANOVA to compare multiple **dependent variables** simultaneously.  \n",
        " *Example:* Evaluating how a marketing campaign influences **brand awareness and customer satisfaction** at the same time.  \n",
        "\n",
        "Q22 What is the F-test, and how does it relate to hypothesis testing ?\n",
        "\n",
        "The **F-test** is a statistical test used to compare the **variances** of two datasets or to evaluate the significance of multiple regression models. It plays a crucial role in hypothesis testing, particularly in **ANOVA (Analysis of Variance)** and **variance comparison tests**.\n",
        "\n",
        "**How the F-Test Relates to Hypothesis Testing**\n",
        "\n",
        "1. **Compares Variances**:  \n",
        "   - The F-test examines whether two populations have the **same variance** or whether the variance among multiple groups is statistically significant.  \n",
        "\n",
        "2. **Forms Hypotheses**:  \n",
        "   - **Null Hypothesis (H₀):** The variances of the two groups are equal.  \n",
        "   - **Alternative Hypothesis (H₁):** The variances are different.  \n",
        "\n",
        "3. **Computes the F-statistic**:  \n",
        "\\[\n",
        "F = \\frac{ \\text{Variance of Group 1} }{ \\text{Variance of Group 2} }\n",
        "\\]\n",
        "   - If **F > critical value** (from the F-distribution table), H₀ is rejected, indicating a significant variance difference.\n",
        "\n",
        "4. **Used in ANOVA and Regression Analysis**:  \n",
        "   - In **ANOVA**, the F-test determines if group means differ significantly.  \n",
        "   - In **linear regression**, the F-test checks if independent variables meaningfully explain the variation in the dependent variable.\n",
        "\n",
        "**Example Application**\n",
        "\n",
        "A factory tests whether two machines produce products with **equal variance in defect rates**. If the F-test finds a significant difference, managers may adjust quality control processes.\n",
        "\n"
      ],
      "metadata": {
        "id": "3JiTIL4hEDCt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "...\n",
        " **Practical Part-1**\n",
        "\n",
        "#Q1 Write a Python program to generate a random variable and display its value ?\n",
        "\n",
        "import random\n",
        "\n",
        "# Generate a random integer between 1 and 100\n",
        "random_variable = random.randint(1, 100)\n",
        "\n",
        "# Display the random variable\n",
        "print(\"Generated random variable:\", random_variable)\n",
        "...\n",
        "#Q2 Generate a discrete uniform distribution using Python and plot the probability mass function (PMF)?\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import randint\n",
        "\n",
        "# Define the range of the discrete uniform distribution\n",
        "low, high = 1, 10  # Values from 1 to 10\n",
        "rv = randint(low, high+1)  # Discrete uniform distribution\n",
        "\n",
        "# Generate possible values\n",
        "x = np.arange(low, high+1)\n",
        "pmf_values = rv.pmf(x)\n",
        "\n",
        "# Plot the PMF\n",
        "plt.bar(x, pmf_values, color='blue', alpha=0.7)\n",
        "plt.xlabel('Value')\n",
        "plt.ylabel('Probability')\n",
        "plt.title('Probability Mass Function (PMF) of Discrete Uniform Distribution')\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.show()\n",
        "...\n",
        "#Q3 Write a Python function to calculate the probability distribution function (PDF) of a Bernoulli distribution?\n",
        "def bernoulli_pdf(x, p):\n",
        "    \"\"\"\n",
        "    Calculate the probability distribution function (PDF) of a Bernoulli distribution.\n",
        "\n",
        "    Parameters:\n",
        "    x (int): The outcome (0 or 1).\n",
        "    p (float): The probability of success (0 ≤ p ≤ 1).\n",
        "\n",
        "    Returns:\n",
        "    float: Probability associated with the given outcome.\n",
        "    \"\"\"\n",
        "    if x not in [0, 1]:\n",
        "        raise ValueError(\"x must be either 0 or 1 for a Bernoulli distribution.\")\n",
        "    if not (0 <= p <= 1):\n",
        "        raise ValueError(\"Probability p must be between 0 and 1.\")\n",
        "\n",
        "    return p if x == 1 else 1 - p\n",
        "\n",
        "# Example usage:\n",
        "p_success = 0.7\n",
        "print(\"P(X=1):\", bernoulli_pdf(1, p_success))\n",
        "print(\"P(X=0):\", bernoulli_pdf(0, p_success))\n",
        "...\n",
        "#Q4 Write a Python script to simulate a binomial distribution with n=10 and p=0.5, then plot its histogram ?\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define parameters for the binomial distribution\n",
        "n = 10  # Number of trials\n",
        "p = 0.5  # Probability of success\n",
        "size = 1000  # Number of experiments\n",
        "\n",
        "# Generate binomially distributed random values\n",
        "binomial_samples = np.random.binomial(n, p, size)\n",
        "\n",
        "# Plot histogram\n",
        "plt.hist(binomial_samples, bins=range(n+2), density=True, color='blue', alpha=0.7, edgecolor='black')\n",
        "plt.xlabel('Number of successes')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title(f'Binomial Distribution Histogram (n={n}, p={p})')\n",
        "plt.xticks(range(n+1))\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.show()\n",
        "...\n",
        "#Q5 Create a Poisson distribution and visualize it using Python ?\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define parameters for the Poisson distribution\n",
        "lambda_value = 5  # Mean number of occurrences (λ)\n",
        "size = 1000  # Number of experiments\n",
        "\n",
        "# Generate Poisson distributed random values\n",
        "poisson_samples = np.random.poisson(lambda_value, size)\n",
        "\n",
        "# Plot histogram\n",
        "plt.hist(poisson_samples, bins=range(0, max(poisson_samples)+1), density=True, color='purple', alpha=0.7, edgecolor='black')\n",
        "plt.xlabel('Number of occurrences')\n",
        "plt.ylabel('Probability')\n",
        "plt.title(f'Poisson Distribution Histogram (λ={lambda_value})')\n",
        "plt.xticks(range(0, max(poisson_samples)+1))\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.show()\n",
        "...\n",
        "#Q6 Write a Python program to calculate and plot the cumulative distribution function (CDF) of a discrete uniform distribution ?\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import randint\n",
        "\n",
        "# Define the range of the discrete uniform distribution\n",
        "low, high = 1, 10  # Values from 1 to 10\n",
        "rv = randint(low, high+1)  # Discrete uniform distribution\n",
        "\n",
        "# Generate possible values\n",
        "x = np.arange(low, high+1)\n",
        "cdf_values = rv.cdf(x)\n",
        "\n",
        "# Plot the CDF\n",
        "plt.plot(x, cdf_values, marker='o', linestyle='-', color='red')\n",
        "plt.xlabel('Value')\n",
        "plt.ylabel('Cumulative Probability')\n",
        "plt.title('Cumulative Distribution Function (CDF) of Discrete Uniform Distribution')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "...\n",
        "#Q7  Generate a continuous uniform distribution using NumPy and visualize it ?\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Generate random samples from a continuous uniform distribution\n",
        "low, high = 0, 1  # Range [0, 1]\n",
        "size = 1000  # Number of samples\n",
        "uniform_samples = np.random.uniform(low, high, size)\n",
        "\n",
        "# Plot histogram\n",
        "plt.hist(uniform_samples, bins=30, density=True, color='green', alpha=0.7, edgecolor='black')\n",
        "plt.xlabel('Value')\n",
        "plt.ylabel('Density')\n",
        "plt.title('Continuous Uniform Distribution Histogram')\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.show()\n",
        "...\n",
        "#Q8 Simulate data from a normal distribution and plot its histogram ?\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define parameters for the normal distribution\n",
        "mean = 0  # Mean (μ)\n",
        "std_dev = 1  # Standard deviation (σ)\n",
        "size = 1000  # Number of samples\n",
        "\n",
        "# Generate normally distributed random values\n",
        "normal_samples = np.random.normal(mean, std_dev, size)\n",
        "\n",
        "# Plot histogram\n",
        "plt.hist(normal_samples, bins=30, density=True, color='blue', alpha=0.7, edgecolor='black')\n",
        "plt.xlabel('Value')\n",
        "plt.ylabel('Density')\n",
        "plt.title('Normal Distribution Histogram')\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.show()\n",
        "...\n",
        "#Q9 Write a Python function to calculate Z-scores from a dataset and plot them ?\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def calculate_z_scores(data):\n",
        "    \"\"\"\n",
        "    Calculate Z-scores for a given dataset.\n",
        "\n",
        "    Parameters:\n",
        "    data (list or np.array): The dataset.\n",
        "\n",
        "    Returns:\n",
        "    np.array: The Z-scores of the dataset.\n",
        "    \"\"\"\n",
        "    mean = np.mean(data)\n",
        "    std_dev = np.std(data, ddof=1)  # Using ddof=1 for sample standard deviation\n",
        "    return (data - mean) / std_dev\n",
        "\n",
        "# Generate a random dataset\n",
        "np.random.seed(42)\n",
        "data = np.random.normal(loc=50, scale=10, size=100)  # Normal distribution with mean=50, std=10\n",
        "\n",
        "# Calculate Z-scores\n",
        "z_scores = calculate_z_scores(data)\n",
        "\n",
        "# Plot histogram of Z-scores\n",
        "plt.hist(z_scores, bins=20, density=True, color='blue', alpha=0.7, edgecolor='black')\n",
        "plt.xlabel('Z-score')\n",
        "plt.ylabel('Density')\n",
        "plt.title('Histogram of Z-scores')\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.show()\n",
        "...\n",
        "#Q10 Implement the Central Limit Theorem (CLT) using Python for a non-normal distribution ?\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define parameters for the exponential distribution\n",
        "lambda_value = 1  # Rate parameter for the exponential distribution\n",
        "population_size = 10000  # Number of data points\n",
        "sample_size = 30  # Size of each sample\n",
        "num_samples = 1000  # Number of samples\n",
        "\n",
        "# Generate a non-normal population (exponential distribution)\n",
        "population = np.random.exponential(scale=1/lambda_value, size=population_size)\n",
        "\n",
        "# Generate sample means\n",
        "sample_means = [np.mean(np.random.choice(population, sample_size, replace=False)) for _ in range(num_samples)]\n",
        "\n",
        "# Plot histogram of sample means\n",
        "plt.hist(sample_means, bins=30, density=True, color='blue', alpha=0.7, edgecolor='black')\n",
        "plt.xlabel('Sample Mean')\n",
        "plt.ylabel('Density')\n",
        "plt.title(f'Central Limit Theorem (CLT) Demonstration\\n{num_samples} Samples of Size {sample_size}')\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.show()\n",
        "...\n",
        "#Q11 Simulate multiple samples from a normal distribution and verify the Central Limit Theorem ?\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define parameters for the normal distribution\n",
        "mean = 50  # Population mean (μ)\n",
        "std_dev = 15  # Population standard deviation (σ)\n",
        "population_size = 100000  # Large population size\n",
        "sample_size = 30  # Size of each sample\n",
        "num_samples = 1000  # Number of samples\n",
        "\n",
        "# Generate normally distributed population\n",
        "population = np.random.normal(mean, std_dev, population_size)\n",
        "\n",
        "# Generate sample means\n",
        "sample_means = [np.mean(np.random.choice(population, sample_size, replace=False)) for _ in range(num_samples)]\n",
        "\n",
        "# Plot histogram of sample means\n",
        "plt.hist(sample_means, bins=30, density=True, color='blue', alpha=0.7, edgecolor='black')\n",
        "plt.xlabel('Sample Mean')\n",
        "plt.ylabel('Density')\n",
        "plt.title(f'Central Limit Theorem (CLT) Verification\\n{num_samples} Samples of Size {sample_size}')\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.show()\n",
        "...\n",
        "#Q12 Write a Python function to calculate and plot the standard normal distribution (mean = 0, std = 1) ?\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import norm\n",
        "\n",
        "def plot_standard_normal_distribution():\n",
        "    \"\"\"\n",
        "    Generate and plot the standard normal distribution (mean=0, std=1).\n",
        "    \"\"\"\n",
        "    x = np.linspace(-4, 4, 1000)  # Generate values from -4 to 4\n",
        "    y = norm.pdf(x, 0, 1)  # Compute the PDF of the standard normal distribution\n",
        "\n",
        "    # Plot the standard normal distribution\n",
        "    plt.plot(x, y, color='blue', label='Standard Normal Distribution')\n",
        "    plt.fill_between(x, y, alpha=0.2, color='blue')\n",
        "    plt.xlabel('Value')\n",
        "    plt.ylabel('Probability Density')\n",
        "    plt.title('Standard Normal Distribution (μ=0, σ=1)')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "# Call the function to plot the distribution\n",
        "plot_standard_normal_distribution()\n",
        "...\n",
        "#Q13 Generate random variables and calculate their corresponding probabilities using the binomial distribution ?\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import binom\n",
        "\n",
        "# Define parameters for the binomial distribution\n",
        "n = 10  # Number of trials\n",
        "p = 0.5  # Probability of success\n",
        "size = 1000  # Number of random variables\n",
        "\n",
        "# Generate random variables from the binomial distribution\n",
        "binomial_samples = np.random.binomial(n, p, size)\n",
        "\n",
        "# Calculate probability mass function (PMF) for all possible values\n",
        "x = np.arange(n+1)  # Possible values (0 to n)\n",
        "pmf_values = binom.pmf(x, n, p)\n",
        "\n",
        "# Plot histogram of generated random variables\n",
        "plt.hist(binomial_samples, bins=n+1, density=True, color='blue', alpha=0.7, edgecolor='black', label=\"Simulated Data\")\n",
        "\n",
        "# Overlay the theoretical PMF\n",
        "plt.plot(x, pmf_values, marker='o', linestyle='-', color='red', label=\"Theoretical PMF\")\n",
        "\n",
        "plt.xlabel('Number of Successes')\n",
        "plt.ylabel('Probability')\n",
        "plt.title(f'Binomial Distribution (n={n}, p={p})')\n",
        "plt.legend()\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.show()\n",
        "...\n",
        "#Q14 Write a Python program to calculate the Z-score for a given data point and compare it to a standard normal distribution ?\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import norm\n",
        "\n",
        "def calculate_z_score(x, mean, std_dev):\n",
        "    \"\"\"\n",
        "    Calculate the Z-score for a given data point.\n",
        "\n",
        "    Parameters:\n",
        "    x (float): The data point.\n",
        "    mean (float): The mean of the dataset.\n",
        "    std_dev (float): The standard deviation of the dataset.\n",
        "\n",
        "    Returns:\n",
        "    float: The Z-score.\n",
        "    \"\"\"\n",
        "    return (x - mean) / std_dev\n",
        "\n",
        "# Example dataset parameters\n",
        "mean = 50\n",
        "std_dev = 10\n",
        "data_point = 65  # Example data point\n",
        "\n",
        "# Calculate Z-score\n",
        "z_score = calculate_z_score(data_point, mean, std_dev)\n",
        "print(f\"Z-score for data point {data_point}: {z_score:.2f}\")\n",
        "\n",
        "# Generate standard normal distribution\n",
        "x_values = np.linspace(-4, 4, 1000)\n",
        "y_values = norm.pdf(x_values, 0, 1)\n",
        "\n",
        "# Plot standard normal distribution\n",
        "plt.plot(x_values, y_values, color='blue', label=\"Standard Normal Distribution\")\n",
        "\n",
        "# Highlight the calculated Z-score\n",
        "plt.axvline(z_score, color='red', linestyle='dashed', label=f\"Z-score = {z_score:.2f}\")\n",
        "plt.fill_between(x_values, y_values, where=(x_values <= z_score), color='red', alpha=0.3)\n",
        "\n",
        "plt.xlabel('Z-score')\n",
        "plt.ylabel('Probability Density')\n",
        "plt.title('Comparison of Z-score with Standard Normal Distribution')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "...\n",
        "#Q15 Implement hypothesis testing using Z-statistics for a sample dataset ?\n",
        "import numpy as np\n",
        "import scipy.stats as stats\n",
        "\n",
        "def z_test(sample_data, pop_mean, pop_std, alpha=0.05):\n",
        "\n",
        "    # Calculate sample mean and sample size\n",
        "    sample_mean = np.mean(sample_data)\n",
        "    n = len(sample_data)\n",
        "\n",
        "    # Compute Z-statistic\n",
        "    z_stat = (sample_mean - pop_mean) / (pop_std / np.sqrt(n))\n",
        "\n",
        "    # Compute critical value (for two-tailed test)\n",
        "    z_critical = stats.norm.ppf(1 - alpha / 2)\n",
        "\n",
        "    # Print results\n",
        "    print(f\"Sample Mean: {sample_mean:.2f}\")\n",
        "    print(f\"Z-Statistic: {z_stat:.2f}\")\n",
        "    print(f\"Critical Value (Zα/2): ±{z_critical:.2f}\")\n",
        "\n",
        "    # Hypothesis test decision\n",
        "    if abs(z_stat) > z_critical:\n",
        "        print(\"Result: Reject the null hypothesis (H0).\")\n",
        "    else:\n",
        "        print(\"Result: Fail to reject the null hypothesis (H0).\")\n",
        "\n",
        "# Example usage\n",
        "np.random.seed(42)\n",
        "sample_data = np.random.normal(loc=52, scale=5, size=40)  # Sample dataset\n",
        "pop_mean = 50  # Hypothesized mean\n",
        "pop_std = 5  # Known population standard deviation\n",
        "\n",
        "z_test(sample_data, pop_mean, pop_std)\n",
        "...\n",
        "#Q16 Create a confidence interval for a dataset using Python and interpret the result ?\n",
        "import numpy as np\n",
        "import scipy.stats as stats\n",
        "\n",
        "def confidence_interval(data, confidence=0.95):\n",
        "    \"\"\"\n",
        "    Calculate the confidence interval for a dataset.\n",
        "\n",
        "    Parameters:\n",
        "    data (list or np.array): The dataset.\n",
        "    confidence (float): Confidence level (default: 0.95).\n",
        "\n",
        "    Returns:\n",
        "    tuple: Lower and upper bounds of the confidence interval.\n",
        "    \"\"\"\n",
        "    sample_mean = np.mean(data)\n",
        "    sample_std = np.std(data, ddof=1)  # Sample standard deviation (ddof=1 for unbiased estimate)\n",
        "    n = len(data)\n",
        "\n",
        "    # Compute Z-score\n",
        "    z_critical = stats.norm.ppf(1 - (1 - confidence) / 2)\n",
        "\n",
        "    # Compute margin of error\n",
        "    margin_of_error = z_critical * (sample_std / np.sqrt(n))\n",
        "\n",
        "    # Compute confidence interval\n",
        "    lower_bound = sample_mean - margin_of_error\n",
        "    upper_bound = sample_mean + margin_of_error\n",
        "\n",
        "    return lower_bound, upper_bound\n",
        "\n",
        "# Example dataset\n",
        "np.random.seed(42)\n",
        "sample_data = np.random.normal(loc=100, scale=15, size=50)  # Sample dataset\n",
        "\n",
        "# Calculate 95% confidence interval\n",
        "ci_lower, ci_upper = confidence_interval(sample_data)\n",
        "print(f\"95% Confidence Interval: ({ci_lower:.2f}, {ci_upper:.2f})\")\n",
        "...\n",
        "#Q17 Generate data from a normal distribution, then calculate and interpret the confidence interval for its mean ?\n",
        "import numpy as np\n",
        "import scipy.stats as stats\n",
        "\n",
        "# Step 1: Generate data from a normal distribution\n",
        "np.random.seed(42)  # For reproducibility\n",
        "mean = 100  # True mean\n",
        "std_dev = 15  # True standard deviation\n",
        "size = 50  # Sample size\n",
        "\n",
        "# Generate random samples\n",
        "data = np.random.normal(mean, std_dev, size)\n",
        "\n",
        "# Step 2: Calculate Confidence Interval\n",
        "sample_mean = np.mean(data)\n",
        "sample_std = np.std(data, ddof=1)  # Using ddof=1 for an unbiased estimate\n",
        "confidence = 0.95  # Confidence level\n",
        "\n",
        "# Compute the critical Z-score\n",
        "z_critical = stats.norm.ppf(1 - (1 - confidence) / 2)\n",
        "\n",
        "# Calculate the margin of error\n",
        "margin_of_error = z_critical * (sample_std / np.sqrt(size))\n",
        "\n",
        "# Compute confidence interval bounds\n",
        "lower_bound = sample_mean - margin_of_error\n",
        "upper_bound = sample_mean + margin_of_error\n",
        "\n",
        "# Print results\n",
        "print(f\"Sample Mean: {sample_mean:.2f}\")\n",
        "print(f\"95% Confidence Interval: ({lower_bound:.2f}, {upper_bound:.2f})\")\n",
        "...\n",
        "#Q18 Write a Python script to calculate and visualize the probability density function (PDF) of a normal distribution ?\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import norm\n",
        "\n",
        "# Define parameters for the normal distribution\n",
        "mean = 0  # Mean (μ)\n",
        "std_dev = 1  # Standard deviation (σ)\n",
        "\n",
        "# Generate values along the x-axis\n",
        "x_values = np.linspace(-4, 4, 1000)\n",
        "\n",
        "# Calculate the probability density function (PDF)\n",
        "pdf_values = norm.pdf(x_values, mean, std_dev)\n",
        "\n",
        "# Plot the normal distribution\n",
        "plt.plot(x_values, pdf_values, color='blue', label='Normal Distribution PDF')\n",
        "plt.fill_between(x_values, pdf_values, alpha=0.2, color='blue')\n",
        "plt.xlabel('Value')\n",
        "plt.ylabel('Probability Density')\n",
        "plt.title('Probability Density Function (PDF) of Normal Distribution')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "...\n",
        "#Q19 Use Python to calculate and interpret the cumulative distribution function (CDF) of a Poisson distribution ?\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import poisson\n",
        "\n",
        "# Define parameters\n",
        "lambda_value = 5  # Mean number of occurrences\n",
        "x_values = np.arange(0, 15)  # Possible values of X\n",
        "\n",
        "# Calculate CDF\n",
        "cdf_values = poisson.cdf(x_values, lambda_value)\n",
        "\n",
        "# Plot the CDF\n",
        "plt.step(x_values, cdf_values, where='post', color='red', label='Poisson CDF')\n",
        "plt.xlabel('Number of occurrences (x)')\n",
        "plt.ylabel('Cumulative Probability')\n",
        "plt.title(f'Poisson CDF (λ={lambda_value})')\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.show()\n",
        "...\n",
        "#Q20 Simulate a random variable using a continuous uniform distribution and calculate its expected value ?\n",
        "import numpy as np\n",
        "\n",
        "# Define parameters for the uniform distribution\n",
        "a, b = 0, 10  # Range [0, 10]\n",
        "size = 1000  # Number of random samples\n",
        "\n",
        "# Generate random variable samples\n",
        "uniform_samples = np.random.uniform(a, b, size)\n",
        "\n",
        "# Calculate expected value\n",
        "expected_value = (a + b) / 2\n",
        "\n",
        "# Print results\n",
        "print(f\"Generated {size} samples from U({a}, {b})\")\n",
        "print(f\"Expected Value (Mean): {expected_value:.2f}\")\n",
        "print(f\"Sample Mean: {np.mean(uniform_samples):.2f}\")\n",
        "...\n",
        "#Q21 Write a Python program to compare the standard deviations of two datasets and visualize the difference ?\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Generate two datasets with different spreads\n",
        "np.random.seed(42)\n",
        "dataset1 = np.random.normal(loc=50, scale=5, size=100)  # Low standard deviation\n",
        "dataset2 = np.random.normal(loc=50, scale=15, size=100)  # High standard deviation\n",
        "\n",
        "# Calculate standard deviations\n",
        "std_dev1 = np.std(dataset1, ddof=1)  # Sample standard deviation (ddof=1)\n",
        "std_dev2 = np.std(dataset2, ddof=1)\n",
        "\n",
        "# Print standard deviation results\n",
        "print(f\"Standard Deviation of Dataset 1: {std_dev1:.2f}\")\n",
        "print(f\"Standard Deviation of Dataset 2: {std_dev2:.2f}\")\n",
        "\n",
        "# Visualize the datasets\n",
        "plt.hist(dataset1, bins=20, alpha=0.7, label=f'Dataset 1 (σ={std_dev1:.2f})', color='blue', edgecolor='black')\n",
        "plt.hist(dataset2, bins=20, alpha=0.7, label=f'Dataset 2 (σ={std_dev2:.2f})', color='red', edgecolor='black')\n",
        "\n",
        "plt.xlabel('Value')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Comparison of Standard Deviations')\n",
        "plt.legend()\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.show()\n",
        "...\n",
        "#Q22 Calculate the range and interquartile range (IQR) of a dataset generated from a normal distribution ?\n",
        "import numpy as np\n",
        "\n",
        "# Generate data from a normal distribution\n",
        "np.random.seed(42)\n",
        "data = np.random.normal(loc=100, scale=15, size=100)  # Mean=100, StdDev=15, Sample size=100\n",
        "\n",
        "# Calculate Range\n",
        "data_range = np.max(data) - np.min(data)\n",
        "\n",
        "# Calculate Interquartile Range (IQR)\n",
        "q1 = np.percentile(data, 25)\n",
        "q3 = np.percentile(data, 75)\n",
        "iqr = q3 - q1\n",
        "\n",
        "# Print results\n",
        "print(f\"Range: {data_range:.2f}\")\n",
        "print(f\"Interquartile Range (IQR): {iqr:.2f}\")\n",
        "...\n",
        "#Q23 Implement Z-score normalization on a dataset and visualize its transformation ?\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Generate a dataset\n",
        "np.random.seed(42)\n",
        "data = np.random.normal(loc=50, scale=15, size=100)  # Original dataset\n",
        "\n",
        "# Compute Z-score normalization\n",
        "mean = np.mean(data)\n",
        "std_dev = np.std(data, ddof=1)  # Sample standard deviation\n",
        "normalized_data = (data - mean) / std_dev\n",
        "\n",
        "# Plot before and after normalization\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.hist(data, bins=20, color='blue', alpha=0.7, edgecolor='black')\n",
        "plt.xlabel('Original Values')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Original Data Distribution')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.hist(normalized_data, bins=20, color='red', alpha=0.7, edgecolor='black')\n",
        "plt.xlabel('Z-score Normalized Values')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Z-score Normalized Data Distribution')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "...\n",
        "#Q24 Write a Python function to calculate the skewness and kurtosis of a dataset generated from a normal distribution.?\n",
        "import numpy as np\n",
        "import scipy.stats as stats\n",
        "\n",
        "def calculate_skewness_kurtosis(data):\n",
        "    \"\"\"\n",
        "    Calculate skewness and kurtosis of a dataset.\n",
        "\n",
        "    Parameters:\n",
        "    data (list or np.array): The dataset.\n",
        "\n",
        "    Returns:\n",
        "    tuple: Skewness and kurtosis values.\n",
        "    \"\"\"\n",
        "    skewness = stats.skew(data)\n",
        "    kurtosis = stats.kurtosis(data, fisher=True)  # Fisher=True gives excess kurtosis (subtracts 3)\n",
        "    return skewness, kurtosis\n",
        "\n",
        "# Generate data from a normal distribution\n",
        "np.random.seed(42)\n",
        "data = np.random.normal(loc=100, scale=15, size=1000)\n",
        "\n",
        "# Compute skewness and kurtosis\n",
        "skewness, kurtosis = calculate_skewness_kurtosis(data)\n",
        "print(f\"Skewness: {skewness:.2f}\")\n",
        "print(f\"Kurtosis (Excess): {kurtosis:.2f}\")\n",
        "..."
      ],
      "metadata": {
        "id": "2vqtId00kP_U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "...\n",
        "**Practical Part-2**\n",
        "#Q1 Write a Python program to perform a Z-test for comparing a sample mean to a known population mean and interpret the results ?\n",
        "import numpy as np\n",
        "import scipy.stats as stats\n",
        "\n",
        "def z_test(sample_data, pop_mean, pop_std, alpha=0.05):\n",
        "    \"\"\"\n",
        "    Perform a Z-test for a sample mean compared to a known population mean.\n",
        "\n",
        "    Parameters:\n",
        "    sample_data (list or np.array): Sample dataset.\n",
        "    pop_mean (float): Population mean (hypothesized).\n",
        "    pop_std (float): Population standard deviation (σ).\n",
        "    alpha (float): Significance level (default: 0.05).\n",
        "\n",
        "    Returns:\n",
        "    None (prints hypothesis test results).\n",
        "    \"\"\"\n",
        "\n",
        "    # Compute sample statistics\n",
        "    sample_mean = np.mean(sample_data)\n",
        "    sample_size = len(sample_data)\n",
        "\n",
        "    # Calculate Z-statistic\n",
        "    z_stat = (sample_mean - pop_mean) / (pop_std / np.sqrt(sample_size))\n",
        "\n",
        "    # Compute critical value for two-tailed test\n",
        "    z_critical = stats.norm.ppf(1 - alpha / 2)\n",
        "\n",
        "    # Print results\n",
        "    print(f\"Sample Mean: {sample_mean:.2f}\")\n",
        "    print(f\"Z-Statistic: {z_stat:.2f}\")\n",
        "    print(f\"Critical Value (±Zα/2): {z_critical:.2f}\")\n",
        "\n",
        "    # Hypothesis test decision\n",
        "    if abs(z_stat) > z_critical:\n",
        "        print(\"Result: Reject the null hypothesis (H0).\")\n",
        "    else:\n",
        "        print(\"Result: Fail to reject the null hypothesis (H0).\")\n",
        "\n",
        "# Example usage\n",
        "np.random.seed(42)\n",
        "sample_data = np.random.normal(loc=52, scale=5, size=40)  # Sample dataset\n",
        "pop_mean = 50  # Hypothesized population mean\n",
        "pop_std = 5  # Known population standard deviation\n",
        "\n",
        "z_test(sample_data, pop_mean, pop_std)\n",
        "...\n",
        "#Q2 Simulate random data to perform hypothesis testing and calculate the corresponding P-value using Python ?\n",
        "import numpy as np\n",
        "import scipy.stats as stats\n",
        "\n",
        "# Step 1: Generate two random datasets (simulating two populations)\n",
        "np.random.seed(42)\n",
        "sample1 = np.random.normal(loc=50, scale=10, size=30)  # Mean = 50, StdDev = 10, Sample size = 30\n",
        "sample2 = np.random.normal(loc=55, scale=10, size=30)  # Mean = 55, StdDev = 10, Sample size = 30\n",
        "\n",
        "# Step 2: Perform independent t-test for hypothesis testing\n",
        "t_statistic, p_value = stats.ttest_ind(sample1, sample2)\n",
        "\n",
        "# Step 3: Print results\n",
        "print(f\"T-statistic: {t_statistic:.2f}\")\n",
        "print(f\"P-value: {p_value:.4f}\")\n",
        "\n",
        "# Step 4: Interpretation\n",
        "alpha = 0.05  # Significance level\n",
        "if p_value < alpha:\n",
        "    print(\"Result: Reject the null hypothesis (H0). The two samples are significantly different.\")\n",
        "else:\n",
        "    print(\"Result: Fail to reject the null hypothesis (H0). No significant difference between the samples.\")\n",
        "...\n",
        "#Q3 Implement a one-sample Z-test using Python to compare the sample mean with the population mean ?\n",
        "import numpy as np\n",
        "import scipy.stats as stats\n",
        "\n",
        "def one_sample_z_test(sample_data, pop_mean, pop_std, alpha=0.05):\n",
        "    \"\"\"\n",
        "    Perform a one-sample Z-test.\n",
        "\n",
        "    Parameters:\n",
        "    sample_data (list or np.array): Sample dataset.\n",
        "    pop_mean (float): Population mean (hypothesized).\n",
        "    pop_std (float): Known population standard deviation (σ).\n",
        "    alpha (float): Significance level (default: 0.05).\n",
        "\n",
        "    Returns:\n",
        "    None (prints hypothesis test results).\n",
        "    \"\"\"\n",
        "\n",
        "    # Compute sample statistics\n",
        "    sample_mean = np.mean(sample_data)\n",
        "    sample_size = len(sample_data)\n",
        "\n",
        "    # Calculate Z-statistic\n",
        "    z_stat = (sample_mean - pop_mean) / (pop_std / np.sqrt(sample_size))\n",
        "\n",
        "    # Compute critical value for two-tailed test\n",
        "    z_critical = stats.norm.ppf(1 - alpha / 2)\n",
        "\n",
        "    # Compute P-value\n",
        "    p_value = 2 * (1 - stats.norm.cdf(abs(z_stat)))  # Two-tailed test\n",
        "\n",
        "    # Print results\n",
        "    print(f\"Sample Mean: {sample_mean:.2f}\")\n",
        "    print(f\"Z-Statistic: {z_stat:.2f}\")\n",
        "    print(f\"Critical Value (±Zα/2): {z_critical:.2f}\")\n",
        "    print(f\"P-value: {p_value:.4f}\")\n",
        "\n",
        "    # Hypothesis test decision\n",
        "    if abs(z_stat) > z_critical:\n",
        "        print(\"Result: Reject the null hypothesis (H0). The sample mean is significantly different from the population mean.\")\n",
        "    else:\n",
        "        print(\"Result: Fail to reject the null hypothesis (H0). No significant difference detected.\")\n",
        "\n",
        "# Example usage\n",
        "np.random.seed(42)\n",
        "sample_data = np.random.normal(loc=52, scale=5, size=40)  # Sample dataset\n",
        "pop_mean = 50  # Hypothesized population mean\n",
        "pop_std = 5  # Known population standard deviation\n",
        "\n",
        "one_sample_z_test(sample_data, pop_mean, pop_std)\n",
        "...\n",
        "#Q4 Perform a two-tailed Z-test using Python and visualize the decision region on a plot ?\n",
        "import numpy as np\n",
        "import scipy.stats as stats\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def two_tailed_z_test(sample_data, pop_mean, pop_std, alpha=0.05):\n",
        "\n",
        "    # Compute sample statistics\n",
        "    sample_mean = np.mean(sample_data)\n",
        "    sample_size = len(sample_data)\n",
        "\n",
        "    # Calculate Z-statistic\n",
        "    z_stat = (sample_mean - pop_mean) / (pop_std / np.sqrt(sample_size))\n",
        "\n",
        "    # Compute critical values for a two-tailed test\n",
        "    z_critical = stats.norm.ppf(1 - alpha / 2)\n",
        "\n",
        "    # Compute P-value\n",
        "    p_value = 2 * (1 - stats.norm.cdf(abs(z_stat)))  # Two-tailed test\n",
        "\n",
        "    # Print results\n",
        "    print(f\"Sample Mean: {sample_mean:.2f}\")\n",
        "    print(f\"Z-Statistic: {z_stat:.2f}\")\n",
        "    print(f\"Critical Value (±Zα/2): {z_critical:.2f}\")\n",
        "    print(f\"P-value: {p_value:.4f}\")\n",
        "\n",
        "    # Decision rule\n",
        "    if abs(z_stat) > z_critical:\n",
        "        print(\"Result: Reject the null hypothesis (H0).\")\n",
        "    else:\n",
        "        print(\"Result: Fail to reject the null hypothesis (H0).\")\n",
        "\n",
        "    # Visualization of decision region\n",
        "    x_values = np.linspace(-4, 4, 1000)\n",
        "    y_values = stats.norm.pdf(x_values, 0, 1)\n",
        "\n",
        "    plt.plot(x_values, y_values, color='blue', label=\"Standard Normal Distribution\")\n",
        "    plt.axvline(-z_critical, color='red', linestyle='dashed', label=f\"Critical Value (-Zα/2)\")\n",
        "    plt.axvline(z_critical, color='red', linestyle='dashed', label=f\"Critical Value (Zα/2)\")\n",
        "    plt.axvline(z_stat, color='black', linestyle='solid', label=f\"Z-Statistic ({z_stat:.2f})\")\n",
        "    plt.fill_between(x_values, y_values, where=(x_values < -z_critical) | (x_values > z_critical), color='red', alpha=0.3)\n",
        "\n",
        "    plt.xlabel(\"Z-score\")\n",
        "    plt.ylabel(\"Probability Density\")\n",
        "    plt.title(\"Two-Tailed Z-Test Decision Region\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "np.random.seed(42)\n",
        "sample_data = np.random.normal(loc=52, scale=5, size=40)  # Sample dataset\n",
        "pop_mean = 50  # Hypothesized population mean\n",
        "pop_std = 5  # Known population standard deviation\n",
        "\n",
        "two_tailed_z_test(sample_data, pop_mean, pop_std)\n",
        "...\n",
        "#Q5 Create a Python function that calculates and visualizes Type 1 and Type 2 errors during hypothesis testing ?\n",
        "import numpy as np\n",
        "import scipy.stats as stats\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def visualize_type1_type2_errors(mu_null, mu_alt, sigma, alpha=0.05, n=30):\n",
        "    \"\"\"\n",
        "    Simulates hypothesis testing and visualizes Type 1 and Type 2 errors.\n",
        "\n",
        "    Parameters:\n",
        "    mu_null (float): Hypothesized population mean (H0).\n",
        "    mu_alt (float): Actual population mean (H1).\n",
        "    sigma (float): Population standard deviation.\n",
        "    alpha (float): Significance level (default: 0.05).\n",
        "    n (int): Sample size.\n",
        "    \"\"\"\n",
        "\n",
        "    # Compute critical value for Type 1 error (false positive)\n",
        "    z_critical = stats.norm.ppf(1 - alpha)\n",
        "    critical_value = mu_null + z_critical * (sigma / np.sqrt(n))\n",
        "\n",
        "    # Compute Type 2 error probability (beta)\n",
        "    beta = stats.norm.cdf(critical_value, loc=mu_alt, scale=sigma / np.sqrt(n))\n",
        "    power = 1 - beta  # Power of the test\n",
        "\n",
        "    # Generate normal distribution curves\n",
        "    x_values = np.linspace(mu_null - 3*sigma, mu_alt + 3*sigma, 1000)\n",
        "    y_null = stats.norm.pdf(x_values, mu_null, sigma / np.sqrt(n))\n",
        "    y_alt = stats.norm.pdf(x_values, mu_alt, sigma / np.sqrt(n))\n",
        "\n",
        "    # Plot distributions\n",
        "    plt.plot(x_values, y_null, label=\"Null Distribution (H0)\", color='blue')\n",
        "    plt.plot(x_values, y_alt, label=\"Alternative Distribution (H1)\", color='red')\n",
        "\n",
        "    # Highlight Type 1 Error Region\n",
        "    plt.fill_between(x_values, y_null, where=(x_values >= critical_value), color='blue', alpha=0.3, label=\"Type 1 Error (α)\")\n",
        "\n",
        "    # Highlight Type 2 Error Region\n",
        "    plt.fill_between(x_values, y_alt, where=(x_values < critical_value), color='red', alpha=0.3, label=\"Type 2 Error (β)\")\n",
        "\n",
        "    plt.axvline(critical_value, color='black', linestyle=\"dashed\", label=\"Critical Value\")\n",
        "    plt.xlabel(\"Value\")\n",
        "    plt.ylabel(\"Probability Density\")\n",
        "    plt.title(\"Type 1 and Type 2 Error Visualization\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "    print(f\"Critical Value: {critical_value:.2f}\")\n",
        "    print(f\"Type 1 Error (α): {alpha:.2f}\")\n",
        "    print(f\"Type 2 Error (β): {beta:.2f}\")\n",
        "    print(f\"Test Power (1 - β): {power:.2f}\")\n",
        "\n",
        "# Example usage\n",
        "visualize_type1_type2_errors(mu_null=50, mu_alt=55, sigma=10, alpha=0.05, n=30)\n",
        "...\n",
        "#Q6 Write a Python program to perform an independent T-test and interpret the results ?\n",
        "import numpy as np\n",
        "import scipy.stats as stats\n",
        "\n",
        "def independent_t_test(sample1, sample2, alpha=0.05):\n",
        "    \"\"\"\n",
        "    Perform an independent T-test to compare two sample means.\n",
        "\n",
        "    Parameters:\n",
        "    sample1 (list or np.array): First dataset.\n",
        "    sample2 (list or np.array): Second dataset.\n",
        "    alpha (float): Significance level (default: 0.05).\n",
        "\n",
        "    Returns:\n",
        "    None (prints test results).\n",
        "    \"\"\"\n",
        "\n",
        "    # Perform an independent T-test\n",
        "    t_statistic, p_value = stats.ttest_ind(sample1, sample2)\n",
        "\n",
        "    # Print results\n",
        "    print(f\"T-statistic: {t_statistic:.2f}\")\n",
        "    print(f\"P-value: {p_value:.4f}\")\n",
        "\n",
        "    # Interpretation\n",
        "    if p_value < alpha:\n",
        "        print(\"Result: Reject the null hypothesis (H0). There is a significant difference between the two samples.\")\n",
        "    else:\n",
        "        print(\"Result: Fail to reject the null hypothesis (H0). No significant difference detected.\")\n",
        "\n",
        "# Example usage\n",
        "np.random.seed(42)\n",
        "sample1 = np.random.normal(loc=50, scale=10, size=30)  # Group 1\n",
        "sample2 = np.random.normal(loc=55, scale=10, size=30)  # Group 2\n",
        "\n",
        "independent_t_test(sample1, sample2)\n",
        "...\n",
        "#Q7  Perform a paired sample T-test using Python and visualize the comparison results ?\n",
        "import numpy as np\n",
        "import scipy.stats as stats\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def paired_t_test(pre_data, post_data, alpha=0.05):\n",
        "    \"\"\"\n",
        "    Perform a paired sample T-test and visualize the comparison.\n",
        "\n",
        "    Parameters:\n",
        "    pre_data (list or np.array): First dataset (before intervention).\n",
        "    post_data (list or np.array): Second dataset (after intervention).\n",
        "    alpha (float): Significance level (default: 0.05).\n",
        "\n",
        "    Returns:\n",
        "    None (prints test results and shows visualization).\n",
        "    \"\"\"\n",
        "\n",
        "    # Compute paired differences\n",
        "    differences = np.array(post_data) - np.array(pre_data)\n",
        "\n",
        "    # Perform the paired sample T-test\n",
        "    t_statistic, p_value = stats.ttest_rel(pre_data, post_data)\n",
        "\n",
        "    # Print results\n",
        "    print(f\"Mean of Differences: {np.mean(differences):.2f}\")\n",
        "    print(f\"T-statistic: {t_statistic:.2f}\")\n",
        "    print(f\"P-value: {p_value:.4f}\")\n",
        "\n",
        "    # Decision rule\n",
        "    if p_value < alpha:\n",
        "        print(\"Result: Reject the null hypothesis (H0). The two samples are significantly different.\")\n",
        "    else:\n",
        "        print(\"Result: Fail to reject the null hypothesis (H0). No significant difference detected.\")\n",
        "\n",
        "    # Visualization: Histogram of differences\n",
        "    plt.figure(figsize=(12, 5))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.hist(differences, bins=15, color='purple', alpha=0.7, edgecolor='black')\n",
        "    plt.xlabel(\"Difference (Post - Pre)\")\n",
        "    plt.ylabel(\"Frequency\")\n",
        "    plt.title(\"Histogram of Differences\")\n",
        "\n",
        "    # Visualization: Boxplot comparison\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.boxplot([pre_data, post_data], labels=[\"Pre-Test\", \"Post-Test\"], patch_artist=True)\n",
        "    plt.title(\"Boxplot Comparison of Paired Samples\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Example usage: Simulate pre and post test scores\n",
        "np.random.seed(42)\n",
        "pre_scores = np.random.normal(loc=50, scale=10, size=30)  # Pre-test scores\n",
        "post_scores = pre_scores + np.random.normal(loc=2, scale=5, size=30)  # Post-test scores with slight increase\n",
        "\n",
        "paired_t_test(pre_scores, post_scores)\n",
        "...\n",
        "#Q8 Simulate data and perform both Z-test and T-test, then compare the results using Python ?\n",
        "import numpy as np\n",
        "import scipy.stats as stats\n",
        "\n",
        "def perform_tests(sample_data, pop_mean, pop_std_known, alpha=0.05):\n",
        "\n",
        "    sample_mean = np.mean(sample_data)\n",
        "    sample_std = np.std(sample_data, ddof=1)  # Sample standard deviation\n",
        "    n = len(sample_data)\n",
        "\n",
        "    # Perform Z-test (assuming known population standard deviation)\n",
        "    z_stat = (sample_mean - pop_mean) / (pop_std_known / np.sqrt(n))\n",
        "    z_p_value = 2 * (1 - stats.norm.cdf(abs(z_stat)))  # Two-tailed test\n",
        "\n",
        "    # Perform T-test (assuming unknown population standard deviation)\n",
        "    t_stat, t_p_value = stats.ttest_1samp(sample_data, pop_mean)\n",
        "\n",
        "    # Print results\n",
        "    print(\"=== Z-Test (Known σ) ===\")\n",
        "    print(f\"Z-Statistic: {z_stat:.2f}\")\n",
        "    print(f\"P-value: {z_p_value:.4f}\")\n",
        "    print(\"Decision:\", \"Reject H0\" if z_p_value < alpha else \"Fail to reject H0\")\n",
        "\n",
        "    print(\"\\n=== T-Test (Unknown σ) ===\")\n",
        "    print(f\"T-Statistic: {t_stat:.2f}\")\n",
        "    print(f\"P-value: {t_p_value:.4f}\")\n",
        "    print(\"Decision:\", \"Reject H0\" if t_p_value < alpha else \"Fail to reject H0\")\n",
        "\n",
        "# Example usage: Generate random dataset\n",
        "np.random.seed(42)\n",
        "sample_data = np.random.normal(loc=52, scale=5, size=30)  # Sample dataset\n",
        "pop_mean = 50  # Hypothesized population mean\n",
        "pop_std_known = 5  # Known population standard deviation (for Z-test)\n",
        "\n",
        "perform_tests(sample_data, pop_mean, pop_std_known)\n",
        "...\n",
        "#Q9 Write a Python function to calculate the confidence interval for a sample mean and explain its significance ?\n",
        "import numpy as np\n",
        "import scipy.stats as stats\n",
        "\n",
        "def confidence_interval(data, confidence=0.95):\n",
        "    \"\"\"\n",
        "    Calculate the confidence interval for a sample mean.\n",
        "\n",
        "    Parameters:\n",
        "    data (list or np.array): The dataset.\n",
        "    confidence (float): Confidence level (default: 0.95).\n",
        "\n",
        "    Returns:\n",
        "    tuple: Lower and upper bounds of the confidence interval.\n",
        "    \"\"\"\n",
        "    sample_mean = np.mean(data)\n",
        "    sample_std = np.std(data, ddof=1)  # ddof=1 for unbiased sample standard deviation\n",
        "    n = len(data)\n",
        "\n",
        "    # Compute Z-score for the given confidence level\n",
        "    z_critical = stats.norm.ppf(1 - (1 - confidence) / 2)\n",
        "\n",
        "    # Compute margin of error\n",
        "    margin_of_error = z_critical * (sample_std / np.sqrt(n))\n",
        "\n",
        "    # Compute confidence interval bounds\n",
        "    lower_bound = sample_mean - margin_of_error\n",
        "    upper_bound = sample_mean + margin_of_error\n",
        "\n",
        "    return lower_bound, upper_bound\n",
        "\n",
        "# Example usage\n",
        "np.random.seed(42)\n",
        "sample_data = np.random.normal(loc=100, scale=15, size=50)  # Simulated dataset\n",
        "\n",
        "# Compute confidence interval\n",
        "ci_lower, ci_upper = confidence_interval(sample_data)\n",
        "print(f\"95% Confidence Interval: ({ci_lower:.2f}, {ci_upper:.2f})\")\n",
        "...\n",
        "#Q10 Write a Python program to calculate the margin of error for a given confidence level using sample data ?\n",
        "import numpy as np\n",
        "import scipy.stats as stats\n",
        "\n",
        "def margin_of_error(data, confidence=0.95):\n",
        "    \"\"\"\n",
        "    Calculate the margin of error for a given confidence level using sample data.\n",
        "\n",
        "    Parameters:\n",
        "    data (list or np.array): Sample dataset.\n",
        "    confidence (float): Confidence level (default: 0.95).\n",
        "\n",
        "    Returns:\n",
        "    float: The calculated margin of error.\n",
        "    \"\"\"\n",
        "    sample_std = np.std(data, ddof=1)  # Unbiased sample standard deviation\n",
        "    n = len(data)\n",
        "\n",
        "    # Compute Z-score for the confidence level\n",
        "    z_critical = stats.norm.ppf(1 - (1 - confidence) / 2)\n",
        "\n",
        "    # Compute margin of error\n",
        "    moe = z_critical * (sample_std / np.sqrt(n))\n",
        "    return moe\n",
        "\n",
        "# Example usage\n",
        "np.random.seed(42)\n",
        "sample_data = np.random.normal(loc=100, scale=15, size=50)  # Simulated dataset\n",
        "\n",
        "# Calculate margin of error\n",
        "moe_value = margin_of_error(sample_data)\n",
        "print(f\"Margin of Error (95% Confidence): ±{moe_value:.2f}\")\n",
        "...\n",
        "#Q11 Implement a Bayesian inference method using Bayes' Theorem in Python and explain the process ?\n",
        "import numpy as np\n",
        "\n",
        "def bayesian_inference(prior, likelihood, marginal):\n",
        "    \"\"\"\n",
        "    Compute Bayesian inference using Bayes' Theorem.\n",
        "\n",
        "    Parameters:\n",
        "    prior (float): Prior probability (P(H)).\n",
        "    likelihood (float): Likelihood of data given hypothesis (P(D|H)).\n",
        "    marginal (float): Marginal probability of data (P(D)).\n",
        "\n",
        "    Returns:\n",
        "    float: Posterior probability (P(H|D)).\n",
        "    \"\"\"\n",
        "    posterior = (likelihood * prior) / marginal\n",
        "    return posterior\n",
        "\n",
        "# Example scenario:\n",
        "# Let's say we're diagnosing a disease:\n",
        "# P(Disease) = 0.01 (Prior probability of having the disease)\n",
        "# P(Positive Test | Disease) = 0.95 (Likelihood)\n",
        "# P(Positive Test) = 0.05 (Marginal probability based on overall test outcomes)\n",
        "\n",
        "prior = 0.01\n",
        "likelihood = 0.95\n",
        "marginal = 0.05\n",
        "\n",
        "posterior = bayesian_inference(prior, likelihood, marginal)\n",
        "print(f\"Posterior Probability of Disease Given Positive Test: {posterior:.4f}\")\n",
        "...\n",
        "#Q12 Perform a Chi-square test for independence between two categorical variables in Python ?\n",
        "import numpy as np\n",
        "import scipy.stats as stats\n",
        "\n",
        "# Define observed frequency table (contingency table)\n",
        "# Example: Survey on product preference based on gender\n",
        "#            Product A  Product B\n",
        "# Male        30        10\n",
        "# Female      20        40\n",
        "\n",
        "observed = np.array([[30, 10], [20, 40]])\n",
        "\n",
        "# Perform Chi-square test for independence\n",
        "chi2_stat, p_value, dof, expected = stats.chi2_contingency(observed)\n",
        "\n",
        "# Print results\n",
        "print(f\"Chi-square Statistic: {chi2_stat:.2f}\")\n",
        "print(f\"Degrees of Freedom: {dof}\")\n",
        "print(f\"P-value: {p_value:.4f}\")\n",
        "\n",
        "# Decision rule\n",
        "alpha = 0.05  # Significance level\n",
        "if p_value < alpha:\n",
        "    print(\"Result: Reject the null hypothesis (H0). The variables are dependent.\")\n",
        "else:\n",
        "    print(\"Result: Fail to reject the null hypothesis (H0). The variables are independent.\")\n",
        "\n",
        "# Print expected frequencies\n",
        "print(\"\\nExpected Frequencies:\")\n",
        "print(expected)\n",
        "...\n",
        "#Q13  Write a Python program to calculate the expected frequencies for a Chi-square test based on observed data ?\n",
        "import numpy as np\n",
        "import scipy.stats as stats\n",
        "\n",
        "def calculate_expected_frequencies(observed):\n",
        "    \"\"\"\n",
        "    Compute expected frequencies for a Chi-square test based on observed data.\n",
        "\n",
        "    Parameters:\n",
        "    observed (np.array): Contingency table with observed frequencies.\n",
        "\n",
        "    Returns:\n",
        "    np.array: Expected frequencies table.\n",
        "    \"\"\"\n",
        "    # Perform Chi-square test to get expected frequencies\n",
        "    _, _, _, expected = stats.chi2_contingency(observed)\n",
        "    return expected\n",
        "\n",
        "# Example observed frequency table\n",
        "#            Product A  Product B\n",
        "# Male        30        10\n",
        "# Female      20        40\n",
        "observed_data = np.array([[30, 10], [20, 40]])\n",
        "\n",
        "# Calculate expected frequencies\n",
        "expected_frequencies = calculate_expected_frequencies(observed_data)\n",
        "\n",
        "# Print results\n",
        "print(\"Observed Frequencies:\")\n",
        "print(observed_data)\n",
        "print(\"\\nExpected Frequencies:\")\n",
        "print(np.round(expected_frequencies, 2))  # Rounded for readability\n",
        "...\n",
        "#Q14  Perform a goodness-of-fit test using Python to compare the observed data to an expected distribution.?\n",
        "import numpy as np\n",
        "import scipy.stats as stats\n",
        "\n",
        "def goodness_of_fit_test(observed, expected, alpha=0.05):\n",
        "    \"\"\"\n",
        "    Perform a Chi-square goodness-of-fit test.\n",
        "\n",
        "    Parameters:\n",
        "    observed (list or np.array): Observed frequencies.\n",
        "    expected (list or np.array): Expected frequencies.\n",
        "    alpha (float): Significance level (default: 0.05).\n",
        "\n",
        "    Returns:\n",
        "    None (prints test results).\n",
        "    \"\"\"\n",
        "    # Perform Chi-square goodness-of-fit test\n",
        "    chi2_stat, p_value = stats.chisquare(observed, expected)\n",
        "\n",
        "    # Print results\n",
        "    print(f\"Chi-square Statistic: {chi2_stat:.2f}\")\n",
        "    print(f\"P-value: {p_value:.4f}\")\n",
        "\n",
        "    # Decision rule\n",
        "    if p_value < alpha:\n",
        "        print(\"Result: Reject the null hypothesis (H0). The observed data does NOT fit the expected distribution.\")\n",
        "    else:\n",
        "        print(\"Result: Fail to reject the null hypothesis (H0). The observed data fits the expected distribution.\")\n",
        "\n",
        "# Example observed and expected frequencies\n",
        "observed_data = np.array([25, 30, 45])  # Sample observed counts\n",
        "expected_data = np.array([33, 33, 34])  # Hypothetical expected distribution\n",
        "\n",
        "goodness_of_fit_test(observed_data, expected_data)\n",
        "...\n",
        "#Q15 Create a Python script to simulate and visualize the Chi-square distribution and discuss its characteristics ?\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy.stats as stats\n",
        "\n",
        "def visualize_chi_square(df_values, num_samples=1000):\n",
        "    \"\"\"\n",
        "    Simulate and visualize the Chi-square distribution for different degrees of freedom.\n",
        "\n",
        "    Parameters:\n",
        "    df_values (list): List of degrees of freedom values.\n",
        "    num_samples (int): Number of random samples (default: 1000).\n",
        "    \"\"\"\n",
        "    x_values = np.linspace(0, max(df_values) * 3, 1000)  # X-axis range\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "\n",
        "    for df in df_values:\n",
        "        chi_square_dist = stats.chi2.pdf(x_values, df)\n",
        "        plt.plot(x_values, chi_square_dist, label=f\"df={df}\")\n",
        "\n",
        "    plt.xlabel(\"Value\")\n",
        "    plt.ylabel(\"Probability Density\")\n",
        "    plt.title(\"Chi-square Distribution for Different Degrees of Freedom\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "# Example Usage: Simulate Chi-square distributions for df = [2, 5, 10]\n",
        "visualize_chi_square(df_values=[2, 5, 10])\n",
        "...\n",
        "#Q16 Implement an F-test using Python to compare the variances of two random samples ?\n",
        "import numpy as np\n",
        "import scipy.stats as stats\n",
        "\n",
        "def f_test(sample1, sample2, alpha=0.05):\n",
        "    \"\"\"\n",
        "    Perform an F-test to compare variances of two samples.\n",
        "\n",
        "    Parameters:\n",
        "    sample1 (np.array): First dataset.\n",
        "    sample2 (np.array): Second dataset.\n",
        "    alpha (float): Significance level (default: 0.05).\n",
        "\n",
        "    Returns:\n",
        "    None (prints test results).\n",
        "    \"\"\"\n",
        "\n",
        "    # Compute sample variances\n",
        "    var1 = np.var(sample1, ddof=1)  # Unbiased variance (ddof=1)\n",
        "    var2 = np.var(sample2, ddof=1)\n",
        "\n",
        "    # Compute F-statistic\n",
        "    f_stat = var1 / var2 if var1 > var2 else var2 / var1  # Ensure F > 1 for correct interpretation\n",
        "    df1, df2 = len(sample1) - 1, len(sample2) - 1  # Degrees of freedom\n",
        "\n",
        "    # Compute P-value\n",
        "    p_value = 2 * min(stats.f.cdf(f_stat, df1, df2), 1 - stats.f.cdf(f_stat, df1, df2))  # Two-tailed test\n",
        "\n",
        "    # Print results\n",
        "    print(f\"F-statistic: {f_stat:.2f}\")\n",
        "    print(f\"Degrees of Freedom: ({df1}, {df2})\")\n",
        "    print(f\"P-value: {p_value:.4f}\")\n",
        "\n",
        "    # Decision rule\n",
        "    if p_value < alpha:\n",
        "        print(\"Result: Reject the null hypothesis (H0). The variances are significantly different.\")\n",
        "    else:\n",
        "        print(\"Result: Fail to reject the null hypothesis (H0). No significant difference in variances.\")\n",
        "\n",
        "# Example usage\n",
        "np.random.seed(42)\n",
        "sample1 = np.random.normal(loc=100, scale=15, size=30)  # Sample 1\n",
        "sample2 = np.random.normal(loc=100, scale=25, size=30)  # Sample 2 (higher variance)\n",
        "\n",
        "f_test(sample1, sample2)\n",
        "...\n",
        "#Q17  Write a Python program to perform an ANOVA test to compare means between multiple groups and interpret the results ?\n",
        "import numpy as np\n",
        "import scipy.stats as stats\n",
        "\n",
        "def anova_test(*groups, alpha=0.05):\n",
        "    \"\"\"\n",
        "    Perform an ANOVA test to compare means across multiple groups.\n",
        "\n",
        "    Parameters:\n",
        "    groups (tuple of np.array): Multiple datasets.\n",
        "    alpha (float): Significance level (default: 0.05).\n",
        "\n",
        "    Returns:\n",
        "    None (prints test results).\n",
        "    \"\"\"\n",
        "    # Perform one-way ANOVA\n",
        "    f_statistic, p_value = stats.f_oneway(*groups)\n",
        "\n",
        "    # Print results\n",
        "    print(f\"F-statistic: {f_statistic:.2f}\")\n",
        "    print(f\"P-value: {p_value:.4f}\")\n",
        "\n",
        "    # Interpretation\n",
        "    if p_value < alpha:\n",
        "        print(\"Result: Reject the null hypothesis (H0). At least one group mean is significantly different.\")\n",
        "    else:\n",
        "        print(\"Result: Fail to reject the null hypothesis (H0). No significant difference detected.\")\n",
        "\n",
        "# Example usage: Generate 3 sample groups\n",
        "np.random.seed(42)\n",
        "group1 = np.random.normal(loc=50, scale=10, size=30)\n",
        "group2 = np.random.normal(loc=55, scale=10, size=30)\n",
        "group3 = np.random.normal(loc=60, scale=10, size=30)\n",
        "\n",
        "anova_test(group1, group2, group3)\n",
        "...\n",
        "#Q18 Perform a one-way ANOVA test using Python to compare the means of different groups and plot the results ?\n",
        "import numpy as np\n",
        "import scipy.stats as stats\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def one_way_anova_test(*groups, alpha=0.05):\n",
        "    \"\"\"\n",
        "    Perform a one-way ANOVA test to compare means across multiple groups and visualize the results.\n",
        "\n",
        "    Parameters:\n",
        "    groups (tuple of np.array): Multiple datasets.\n",
        "    alpha (float): Significance level (default: 0.05).\n",
        "\n",
        "    Returns:\n",
        "    None (prints test results and displays boxplots).\n",
        "    \"\"\"\n",
        "    # Perform one-way ANOVA\n",
        "    f_statistic, p_value = stats.f_oneway(*groups)\n",
        "\n",
        "    # Print results\n",
        "    print(f\"F-statistic: {f_statistic:.2f}\")\n",
        "    print(f\"P-value: {p_value:.4f}\")\n",
        "\n",
        "    # Interpretation\n",
        "    if p_value < alpha:\n",
        "        print(\"Result: Reject the null hypothesis (H0). At least one group mean is significantly different.\")\n",
        "    else:\n",
        "        print(\"Result: Fail to reject the null hypothesis (H0). No significant difference detected.\")\n",
        "\n",
        "    # Visualization: Boxplot comparison\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.boxplot(groups, labels=[f\"Group {i+1}\" for i in range(len(groups))], patch_artist=True)\n",
        "    plt.title(\"Comparison of Group Means using ANOVA\")\n",
        "    plt.xlabel(\"Groups\")\n",
        "    plt.ylabel(\"Values\")\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "# Example usage: Generate 3 sample groups\n",
        "np.random.seed(42)\n",
        "group1 = np.random.normal(loc=50, scale=10, size=30)\n",
        "group2 = np.random.normal(loc=55, scale=10, size=30)\n",
        "group3 = np.random.normal(loc=60, scale=10, size=30)\n",
        "\n",
        "one_way_anova_test(group1, group2, group3)\n",
        "...\n",
        "#Q19  Write a Python function to check the assumptions (normality, independence, and equal variance) for ANOVA ?\n",
        "import numpy as np\n",
        "import scipy.stats as stats\n",
        "\n",
        "def check_anova_assumptions(*groups, alpha=0.05):\n",
        "    \"\"\"\n",
        "    Check assumptions (normality and equal variance) for ANOVA.\n",
        "\n",
        "    Parameters:\n",
        "    groups (tuple of np.array): Multiple datasets (samples).\n",
        "    alpha (float): Significance level (default: 0.05).\n",
        "\n",
        "    Returns:\n",
        "    None (prints assumption test results).\n",
        "    \"\"\"\n",
        "\n",
        "    # Normality Check (Shapiro-Wilk test)\n",
        "    print(\"=== Normality Check ===\")\n",
        "    for i, group in enumerate(groups):\n",
        "        stat, p_value = stats.shapiro(group)\n",
        "        print(f\"Group {i+1}: P-value = {p_value:.4f} → {'Normally Distributed' if p_value > alpha else 'Not Normal'}\")\n",
        "\n",
        "    # Equal Variance Check (Levene’s test)\n",
        "    print(\"\\n=== Equal Variance Check ===\")\n",
        "    stat, p_value = stats.levene(*groups)\n",
        "    print(f\"Levene's Test P-value = {p_value:.4f} → {'Equal Variance' if p_value > alpha else 'Unequal Variance'}\")\n",
        "\n",
        "    print(\"\\nAssumptions Summary:\")\n",
        "    print(\"✅ Data should be normally distributed (Shapiro-Wilk test).\")\n",
        "    print(\"✅ Groups should have similar variance (Levene’s test).\")\n",
        "    print(\"⚠ Independence must be ensured from study design.\")\n",
        "\n",
        "# Example Usage\n",
        "np.random.seed(42)\n",
        "group1 = np.random.normal(loc=50, scale=10, size=30)\n",
        "group2 = np.random.normal(loc=55, scale=10, size=30)\n",
        "group3 = np.random.normal(loc=60, scale=10, size=30)\n",
        "\n",
        "check_anova_assumptions(group1, group2, group3)\n",
        "...\n",
        "#Q20 Perform a two-way ANOVA test using Python to study the interaction between two factors and visualize the results ?\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy.stats as stats\n",
        "import statsmodels.api as sm\n",
        "import statsmodels.formula.api as smf\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Step 1: Simulate a dataset\n",
        "np.random.seed(42)\n",
        "\n",
        "# Create categorical factors\n",
        "factor_A = np.random.choice(['Low', 'High'], size=100)\n",
        "factor_B = np.random.choice(['Type 1', 'Type 2'], size=100)\n",
        "\n",
        "# Generate dependent variable (continuous data) influenced by both factors\n",
        "dependent_var = np.random.normal(loc=50, scale=10, size=100) + (factor_A == 'High') * 5 + (factor_B == 'Type 2') * -3\n",
        "\n",
        "# Create DataFrame\n",
        "df = pd.DataFrame({'Factor_A': factor_A, 'Factor_B': factor_B, 'Dependent_Var': dependent_var})\n",
        "\n",
        "# Step 2: Perform Two-Way ANOVA\n",
        "model = smf.ols('Dependent_Var ~ C(Factor_A) + C(Factor_B) + C(Factor_A):C(Factor_B)', data=df).fit()\n",
        "anova_table = sm.stats.anova_lm(model, typ=2)\n",
        "print(\"\\n=== Two-Way ANOVA Results ===\\n\")\n",
        "print(anova_table)\n",
        "\n",
        "# Step 3: Visualization of Group Means using Boxplots\n",
        "plt.figure(figsize=(10, 5))\n",
        "sns.boxplot(x='Factor_A', y='Dependent_Var', hue='Factor_B', data=df)\n",
        "plt.title('Interaction Effect of Factor A & Factor B on Dependent Variable')\n",
        "plt.xlabel('Factor A')\n",
        "plt.ylabel('Dependent Variable')\n",
        "plt.legend(title='Factor B')\n",
        "plt.show()\n",
        "...\n",
        "#Q21 Write a Python program to visualize the F-distribution and discuss its use in hypothesis testing ?\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy.stats as stats\n",
        "\n",
        "def visualize_f_distribution(df1_values, df2_values, x_limit=5):\n",
        "    \"\"\"\n",
        "    Visualize the F-distribution for different degrees of freedom.\n",
        "\n",
        "    Parameters:\n",
        "    df1_values (list): Numerator degrees of freedom.\n",
        "    df2_values (list): Denominator degrees of freedom.\n",
        "    x_limit (float): Upper limit for X-axis.\n",
        "    \"\"\"\n",
        "    x_values = np.linspace(0, x_limit, 1000)\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "\n",
        "    for df1, df2 in zip(df1_values, df2_values):\n",
        "        f_dist = stats.f.pdf(x_values, df1, df2)\n",
        "        plt.plot(x_values, f_dist, label=f\"F-distribution (df1={df1}, df2={df2})\")\n",
        "\n",
        "    plt.xlabel(\"F-value\")\n",
        "    plt.ylabel(\"Probability Density\")\n",
        "    plt.title(\"F-distribution for Different Degrees of Freedom\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "# Example Usage: Simulate different F-distributions\n",
        "visualize_f_distribution(df1_values=[2, 5, 10], df2_values=[20, 30, 50])\n",
        "...\n",
        "#Q22 Perform a one-way ANOVA test in Python and visualize the results with boxplots to compare group means ?\n",
        "import numpy as np\n",
        "import scipy.stats as stats\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "def one_way_anova_test(*groups, alpha=0.05):\n",
        "    \"\"\"\n",
        "    Perform a one-way ANOVA test to compare means across multiple groups and visualize the results.\n",
        "\n",
        "    Parameters:\n",
        "    groups (tuple of np.array): Multiple datasets.\n",
        "    alpha (float): Significance level (default: 0.05).\n",
        "\n",
        "    Returns:\n",
        "    None (prints test results and displays boxplots).\n",
        "    \"\"\"\n",
        "    # Perform one-way ANOVA\n",
        "    f_statistic, p_value = stats.f_oneway(*groups)\n",
        "\n",
        "    # Print results\n",
        "    print(f\"F-statistic: {f_statistic:.2f}\")\n",
        "    print(f\"P-value: {p_value:.4f}\")\n",
        "\n",
        "    # Interpretation\n",
        "    if p_value < alpha:\n",
        "        print(\"Result: Reject the null hypothesis (H0). At least one group mean is significantly different.\")\n",
        "    else:\n",
        "        print(\"Result: Fail to reject the null hypothesis (H0). No significant difference detected.\")\n",
        "\n",
        "    # Create a DataFrame for visualization\n",
        "    import pandas as pd\n",
        "    data = []\n",
        "    for i, group in enumerate(groups):\n",
        "        for value in group:\n",
        "            data.append([f\"Group {i+1}\", value])\n",
        "\n",
        "    df = pd.DataFrame(data, columns=[\"Group\", \"Value\"])\n",
        "\n",
        "    # Visualization: Boxplot comparison\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.boxplot(x=\"Group\", y=\"Value\", data=df, palette=\"Set2\")\n",
        "    plt.title(\"Comparison of Group Means using ANOVA\")\n",
        "    plt.xlabel(\"Groups\")\n",
        "    plt.ylabel(\"Values\")\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "# Example usage: Generate 3 sample groups\n",
        "np.random.seed(42)\n",
        "group1 = np.random.normal(loc=50, scale=10, size=30)\n",
        "group2 = np.random.normal(loc=55, scale=10, size=30)\n",
        "group3 = np.random.normal(loc=60, scale=10, size=30)\n",
        "\n",
        "one_way_anova_test(group1, group2, group3)\n",
        "...\n",
        "#Q23 Simulate random data from a normal distribution, then perform hypothesis testing to evaluate the means ?\n",
        "import numpy as np\n",
        "import scipy.stats as stats\n",
        "\n",
        "def hypothesis_testing(sample_data, pop_mean, alpha=0.05):\n",
        "    \"\"\"\n",
        "    Perform a one-sample T-test to evaluate the sample mean compared to a population mean.\n",
        "\n",
        "    Parameters:\n",
        "    sample_data (np.array): Simulated dataset.\n",
        "    pop_mean (float): Hypothesized population mean.\n",
        "    alpha (float): Significance level (default: 0.05).\n",
        "\n",
        "    Returns:\n",
        "    None (prints test results).\n",
        "    \"\"\"\n",
        "    sample_mean = np.mean(sample_data)\n",
        "    sample_std = np.std(sample_data, ddof=1)\n",
        "    n = len(sample_data)\n",
        "\n",
        "    # Perform one-sample T-test\n",
        "    t_statistic, p_value = stats.ttest_1samp(sample_data, pop_mean)\n",
        "\n",
        "    # Print results\n",
        "    print(f\"Sample Mean: {sample_mean:.2f}\")\n",
        "    print(f\"T-statistic: {t_statistic:.2f}\")\n",
        "    print(f\"P-value: {p_value:.4f}\")\n",
        "\n",
        "    # Decision rule\n",
        "    if p_value < alpha:\n",
        "        print(\"Result: Reject the null hypothesis (H0). The sample mean is significantly different.\")\n",
        "    else:\n",
        "        print(\"Result: Fail to reject the null hypothesis (H0). No significant difference detected.\")\n",
        "\n",
        "# Example usage: Simulate data from a normal distribution\n",
        "np.random.seed(42)\n",
        "sample_data = np.random.normal(loc=52, scale=5, size=30)  # Mean = 52, StdDev = 5, Sample size = 30\n",
        "pop_mean = 50  # Hypothesized population mean\n",
        "\n",
        "hypothesis_testing(sample_data, pop_mean)\n",
        "...\n",
        "#Q24 Perform a hypothesis test for population variance using a Chi-square distribution and interpret the results ?\n",
        "import numpy as np\n",
        "import scipy.stats as stats\n",
        "\n",
        "def chi_square_variance_test(sample_data, pop_variance, alpha=0.05):\n",
        "    \"\"\"\n",
        "    Perform a Chi-square hypothesis test for population variance.\n",
        "\n",
        "    Parameters:\n",
        "    sample_data (np.array): Sample dataset.\n",
        "    pop_variance (float): Hypothesized population variance.\n",
        "    alpha (float): Significance level (default: 0.05).\n",
        "\n",
        "    Returns:\n",
        "    None (prints test results).\n",
        "    \"\"\"\n",
        "    n = len(sample_data)\n",
        "    sample_variance = np.var(sample_data, ddof=1)  # Unbiased sample variance\n",
        "    chi_square_stat = (n - 1) * sample_variance / pop_variance\n",
        "\n",
        "    # Compute P-value (two-tailed test)\n",
        "    p_value = 2 * min(stats.chi2.cdf(chi_square_stat, df=n-1), 1 - stats.chi2.cdf(chi_square_stat, df=n-1))\n",
        "\n",
        "    # Print results\n",
        "    print(f\"Sample Variance: {sample_variance:.2f}\")\n",
        "    print(f\"Chi-square Statistic: {chi_square_stat:.2f}\")\n",
        "    print(f\"Degrees of Freedom: {n-1}\")\n",
        "    print(f\"P-value: {p_value:.4f}\")\n",
        "\n",
        "    # Decision rule\n",
        "    if p_value < alpha:\n",
        "        print(\"Result: Reject the null hypothesis (H0). The variance is significantly different from the hypothesized value.\")\n",
        "    else:\n",
        "        print(\"Result: Fail to reject the null hypothesis (H0). No significant difference detected.\")\n",
        "\n",
        "# Example usage: Simulate a dataset\n",
        "np.random.seed(42)\n",
        "sample_data = np.random.normal(loc=50, scale=5, size=30)  # Sample dataset\n",
        "hypothesized_variance = 25  # Hypothesized population variance (e.g., σ²=5²)\n",
        "\n",
        "chi_square_variance_test(sample_data, hypothesized_variance)\n",
        "...\n",
        "#Q25  Write a Python script to perform a Z-test for comparing proportions between two datasets or groups ?\n",
        "import numpy as np\n",
        "import scipy.stats as stats\n",
        "\n",
        "def z_test_proportions(successes1, size1, successes2, size2, alpha=0.05):\n",
        "    \"\"\"\n",
        "    Perform a Z-test for comparing proportions between two groups.\n",
        "\n",
        "    Parameters:\n",
        "    successes1 (int): Number of successes in group 1.\n",
        "    size1 (int): Sample size of group 1.\n",
        "    successes2 (int): Number of successes in group 2.\n",
        "    size2 (int): Sample size of group 2.\n",
        "    alpha (float): Significance level (default: 0.05).\n",
        "\n",
        "    Returns:\n",
        "    None (prints test results).\n",
        "    \"\"\"\n",
        "\n",
        "    # Compute sample proportions\n",
        "    p1 = successes1 / size1\n",
        "    p2 = successes2 / size2\n",
        "\n",
        "    # Compute pooled proportion\n",
        "    p_pooled = (successes1 + successes2) / (size1 + size2)\n",
        "\n",
        "    # Compute Z-statistic\n",
        "    z_stat = (p1 - p2) / np.sqrt(p_pooled * (1 - p_pooled) * (1/size1 + 1/size2))\n",
        "\n",
        "    # Compute P-value for two-tailed test\n",
        "    p_value = 2 * (1 - stats.norm.cdf(abs(z_stat)))\n",
        "\n",
        "    # Print results\n",
        "    print(f\"Proportion 1 (p1): {p1:.4f}\")\n",
        "    print(f\"Proportion 2 (p2): {p2:.4f}\")\n",
        "    print(f\"Z-statistic: {z_stat:.2f}\")\n",
        "    print(f\"P-value: {p_value:.4f}\")\n",
        "\n",
        "    # Decision rule\n",
        "    if p_value < alpha:\n",
        "        print(\"Result: Reject the null hypothesis (H0). The proportions are significantly different.\")\n",
        "    else:\n",
        "        print(\"Result: Fail to reject the null hypothesis (H0). No significant difference detected.\")\n",
        "\n",
        "# Example usage\n",
        "z_test_proportions(successes1=45, size1=200, successes2=60, size2=220)\n",
        "...\n",
        "#Q26 Implement an F-test for comparing the variances of two datasets, then interpret and visualize the results ?\n",
        "import numpy as np\n",
        "import scipy.stats as stats\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def f_test_and_visualize(sample1, sample2, alpha=0.05):\n",
        "    \"\"\"\n",
        "    Perform an F-test to compare variances of two datasets and visualize results.\n",
        "\n",
        "    Parameters:\n",
        "    sample1 (np.array): First dataset.\n",
        "    sample2 (np.array): Second dataset.\n",
        "    alpha (float): Significance level (default: 0.05).\n",
        "\n",
        "    Returns:\n",
        "    None (prints test results and displays histogram visualization).\n",
        "    \"\"\"\n",
        "\n",
        "    # Compute sample variances\n",
        "    var1 = np.var(sample1, ddof=1)\n",
        "    var2 = np.var(sample2, ddof=1)\n",
        "\n",
        "    # Compute F-statistic\n",
        "    f_stat = var1 / var2 if var1 > var2 else var2 / var1  # Ensure F > 1\n",
        "    df1, df2 = len(sample1) - 1, len(sample2) - 1  # Degrees of freedom\n",
        "\n",
        "    # Compute P-value for two-tailed test\n",
        "    p_value = 2 * min(stats.f.cdf(f_stat, df1, df2), 1 - stats.f.cdf(f_stat, df1, df2))\n",
        "\n",
        "    # Print results\n",
        "    print(f\"Sample Variance 1: {var1:.2f}\")\n",
        "    print(f\"Sample Variance 2: {var2:.2f}\")\n",
        "    print(f\"F-statistic: {f_stat:.2f}\")\n",
        "    print(f\"P-value: {p_value:.4f}\")\n",
        "    print(f\"Degrees of Freedom: ({df1}, {df2})\")\n",
        "\n",
        "    # Decision rule\n",
        "    if p_value < alpha:\n",
        "        print(\"Result: Reject the null hypothesis (H0). The variances are significantly different.\")\n",
        "    else:\n",
        "        print(\"Result: Fail to reject the null hypothesis (H0). No significant difference in variances.\")\n",
        "\n",
        "    # Visualization: Histogram comparison\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.hist(sample1, bins=15, alpha=0.6, color='blue', label=\"Sample 1\", edgecolor='black')\n",
        "    plt.hist(sample2, bins=15, alpha=0.6, color='red', label=\"Sample 2\", edgecolor='black')\n",
        "    plt.xlabel(\"Value\")\n",
        "    plt.ylabel(\"Frequency\")\n",
        "    plt.title(\"Comparison of Distributions (F-test)\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "# Example usage: Generate two sample datasets\n",
        "np.random.seed(42)\n",
        "sample1 = np.random.normal(loc=100, scale=15, size=30)  # Sample 1\n",
        "sample2 = np.random.normal(loc=100, scale=25, size=30)  # Sample 2 (higher variance)\n",
        "\n",
        "f_test_and_visualize(sample1, sample2)\n",
        "...\n",
        "#Q27 Perform a Chi-square test for goodness of fit with simulated data and analyze the results.?\n",
        "import numpy as np\n",
        "import scipy.stats as stats\n",
        "\n",
        "def chi_square_goodness_of_fit(observed, expected, alpha=0.05):\n",
        "    \"\"\"\n",
        "    Perform a Chi-square goodness-of-fit test.\n",
        "\n",
        "    Parameters:\n",
        "    observed (np.array): Observed frequencies.\n",
        "    expected (np.array): Expected frequencies.\n",
        "    alpha (float): Significance level (default: 0.05).\n",
        "\n",
        "    Returns:\n",
        "    None (prints test results).\n",
        "    \"\"\"\n",
        "    # Perform Chi-square goodness-of-fit test\n",
        "    chi2_stat, p_value = stats.chisquare(observed, expected)\n",
        "\n",
        "    # Print results\n",
        "    print(f\"Observed Frequencies: {observed}\")\n",
        "    print(f\"Expected Frequencies: {expected}\")\n",
        "    print(f\"Chi-square Statistic: {chi2_stat:.2f}\")\n",
        "    print(f\"P-value: {p_value:.4f}\")\n",
        "\n",
        "    # Decision rule\n",
        "    if p_value < alpha:\n",
        "        print(\"Result: Reject the null hypothesis (H0). The observed data does NOT fit the expected distribution.\")\n",
        "    else:\n",
        "        print(\"Result: Fail to reject the null hypothesis (H0). The observed data fits the expected distribution.\")\n",
        "\n",
        "# Example simulated data\n",
        "np.random.seed(42)\n",
        "observed_data = np.array([30, 25, 45])  # Example observed counts\n",
        "expected_data = np.array([33, 33, 34])  # Hypothetical expected distribution\n",
        "\n",
        "chi_square_goodness_of_fit(observed_data, expected_data)\n",
        "\n",
        "\n",
        "\n",
        "..."
      ],
      "metadata": {
        "id": "BuL_8-gmq_pk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}